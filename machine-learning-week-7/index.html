
<head>
  <meta http-equiv="Content-Type" content="text/html" charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <title>Machine Learning, Week 7</title>
  <meta name="description" content="">

  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Machine Learning, Week 7">
  <meta name="twitter:description" content="이번시간에 Support Vector Machine, SVM 을 배운다. Optimization Objective 먼저 직관을 얻기 위해 logistic regression 의 sigmoid function 을 좀 보자. (http://blog.csdn.net/abcjennifer) y = 1 이면 0^Tx &gt;&gt; 0 이어야 h(x) 가 1 에 가까워 진다.  이제 cost function 에 h(x) 를 넣자">
  <meta name="twitter:creator" content="@yourTwitterUsername">
  <meta name="twitter:image" content="">
  <meta name="twitter:url" content="http://1ambda.github.io/machine-learning-week-7/">
  <meta name="twitter:domain" content="http://1ambda.github.io">

  

  <link rel="author" href="https://plus.google.com/101105410053351451441?rel=author">

  <link rel="shortcut icon" href="../favicon.ico">

  <link rel="stylesheet" type="text/css" href="http://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css">
  <link rel="stylesheet" type="text/css" href="http://fonts.googleapis.com/css?family=Droid+Serif">
  <link rel="stylesheet" type="text/css" href="http://fonts.googleapis.com/css?family=Open+Sans:600,300">
  <link rel="stylesheet" type="text/css" href="../assets/stylesheets/xpressio.css">
  <link rel="stylesheet" type="text/css" href="../assets/1ambda/1ambda.css">
  <script type="text/javascript" src="../assets/1ambda/modernizr.js">
  </script>
  <script type="text/javascript" src="../assets/1ambda/detectizr.min.js">
  </script>

  <!--load css if windows -->
  <script type="text/javascript">
    if (Modernizr.windows) {
      file = location.pathname.split( "/" ).pop();
      link = document.createElement( "link" );
      link.href = "/assets/1ambda/1ambda_windows.css";
      link.type = "text/css";
      link.rel = "stylesheet";
      link.media = "screen,print";
      document.getElementsByTagName("head")[0].appendChild( link );
    }
  </script>


  
  <link rel="stylesheet" href="../assets/highlight/styles/github.css">
<script src="../assets/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>


  <meta name="generator" content="Ghost 0.5">
<link rel="alternate" type="application/rss+xml" title="Old Lisper" href="../rss">
<link rel="canonical" href="http://1ambda.github.io/machine-learning-week-7/">

  
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-52181619-1', '1ambda.github.io');
  ga('send', 'pageview');
</script>

  
</head>
<body>

  
  <script src="../public/jquery.js?v=6f23f02e25"></script>

  

<header class="site_width text center padding_top_big margin_bottom_big">
  
  <h1 class="blog_title margin_bottom_small"><a href="http://1ambda.github.io">Old Lisper</a></h1>
  <h4 class="text book">Lisp, Emacs, Scala</h4>
  
  <div class="social border solid top_small bottom_small padding_medium">
  <h6 class="text book color c_black_medium without_margin padding_right_big"><a href="/about-me"><i class="fa fa-user"></i> <span class="margin_left_small desktop">About me</span></a></h6>
  <h6 class="text book color c_black_medium without_margin padding_right_big"><a href="http://kr.linkedin.com/in/1ambda" target="_blank"><i class="fa fa-linkedin-square"></i> <span class="margin_left_small desktop">Linkedin</span></a></h6>
  <h6 class="text book color c_black_medium without_margin"><a href="http://github.com/1ambda" target="_blank"><i class="fa fa-github"></i> <span class="margin_left_small desktop">GitHub</span></a></h6>
</div>

</header>

<main class="site_width" role="main">
  <article class="post tag-coursera tag-machine-lerning tag-svm">

    

    <header class="text center margin_bottom_medium">
      <h5 class="text book small uppercase color c_black_light margin_bottom_small">Posted in <a href="../tag/coursera">coursera</a>, <a href="../tag/machine-lerning">machine lerning</a>, <a href="/tag/svm/">SVM</a></h5>
      <h1 class="margin_bottom_medium">Machine Learning, Week 7</h1>
      <h5 class="text book small uppercase color c_black_light margin_bottom_small"><time datetime="2014-11-22">Saturday, November 22, 2014</time>
      <br><br>
       <a href="http://1ambda.github.io/machine-learning-week-7/#disqus_thread">Comment</a>
      </h5>
    </header>

    <section>
      <p>이번시간에 <em>Support Vector Machine, SVM</em> 을 배운다.</p>

<h3 id="optimizationobjective">Optimization Objective</h3>

<p>먼저 직관을 얻기 위해 <em>logistic regression</em> 의 <em>sigmoid function</em> 을 좀 보자.</p>

<p><img src="http://my.csdn.net/uploads/201208/09/1344525027_7041.png" alt=""></p>

<p align="center">(<a href="http://blog.csdn.net/abcjennifer">http://blog.csdn.net/abcjennifer</a>)</p>

<p><code>y = 1</code> 이면 <code>0^Tx &gt;&gt; 0</code> 이어야 <code>h(x)</code> 가 <code>1</code> 에 가까워 진다. </p>

<p>이제 <em>cost function</em> 에 <code>h(x)</code> 를 넣자. 그리고 <code>m = 1</code> 인 트레이닝 셋에 대해서 보면</p>

<p><img src="http://img.my.csdn.net/uploads/201302/14/1360809698_1212.png" alt=""></p>

<p align="center">(<a href="http://blog.csdn.net/abcjennifer">http://blog.csdn.net/abcjennifer</a>)</p>

<p>파란 그래프에서 볼 수 있듯이 <code>y = 1</code> 일때 <code>0^Tx &gt;&gt; 0</code> 이면 <em>cost</em> 가 상당히 낮아지는걸 볼 수 있다. 이 그래프를 좀 단순화 해서 <em>자주색</em> 그래프를 만들어 보자. 두개의 직선으로 만들었는데, 이 <em>cost function</em> 을 계산하면 상당히 근접한 값을 얻을 수 있고, 동시에 그래프가 단순해져 <em>computational advantage</em> 를 얻을 수 있다.</p>

<p>각각 좌측, 우측에 있는 <em>cost function</em> 을 이렇게 쓴다.</p>

<p><img src="http://latex.codecogs.com/gif.latex?%5C%20%5C%5Ccost_1%28z%29%5C%20%28y%20%3D%201%29%20%5C%5Ccost_0%28z%29%5C%20%28y%20%3D%200%29" alt=""></p>

<p><em>logistic regression</em> 식 에서 <code>-log h(x)</code> 를 <code>cost_1(z)</code> 로, <code>-log(1 - h(x)))</code> 를 <code>cost_0(z)</code> 로 바꾸면 </p>

<p><img src="http://latex.codecogs.com/gif.latex?min_%5Ctheta%20%5C%20%7B1%20%5Cover%20m%7D%20%5B%5Csum_%7Bi%3D1%7D%5Em%20y%5E%7B%28i%29%7D%28-log%5C%20h_%5Ctheta%28x%5E%7B%28i%29%7D%29%29%5C%20&amp;plus;%20%5C%20%281%20-%20y%5E%7B%28i%29%7D%29%5C%20%28-log%281%5C%20-%5C%20h_%7B%5Ctheta%7D%28x%5E%7B%28i%29%7D%29%29%29%5D%5C%20&amp;plus;%20%5C%20%7B%5Clambda%20%5Cover%202m%7D%5Csum_%7Bj%3D1%7D%5En%20%5Ctheta_j%5E2%20%5C%5Ccost_1%28z%29%5C%20%28y%20%3D%201%29%20%5C%5Ccost_0%28z%29%5C%20%28y%20%3D%200%29" alt=""></p>

<p><img src="http://latex.codecogs.com/gif.latex?%5C%5Ccost_1%28z%29%5C%20%28y%20%3D%201%29%20%5C%5Ccost_0%28z%29%5C%20%28y%20%3D%200%29%20%5C%5C%20min_%5Ctheta%20%5C%20%7B1%20%5Cover%20m%7D%20%5B%5Csum_%7Bi%3D1%7D%5Em%20y%5E%7B%28i%29%7Dcost_1%28%5Ctheta%5ETx%29%5C%20&amp;plus;%20%5C%20%281%20-%20y%5E%7B%28i%29%7D%29%5C%20%28cost_0%28%5Ctheta%5ETx%29%29%5D%5C%20&amp;plus;%20%5C%20%7B%5Clambda%20%5Cover%202m%7D%5Csum_%7Bj%3D1%7D%5En%20%5Ctheta_j%5E2" alt=""></p>

<p>이 때, <code>1/m</code> 은 상수이므로 제거해도 어차피 똑같은 <code>0(theta)</code> 를 얻을 수 있다.</p>

<p>그리고 식을 좀 간략히 적어보면 </p>

<p><img src="http://latex.codecogs.com/gif.latex?min_%5Ctheta%20%5C%20A%20&amp;plus;%20%5Clambda%20B" alt=""></p>

<p>여기서 <code>lambda</code> 가 하는 일은 <em>low cost ('A')</em> 와 <em>small parameter ('B')</em> 를 조절하는 일이다. 식을 좀 변경하면 이렇게도 볼 수 있다. 여기서 <code>C</code> 는 <code>1 / lambda</code> 과 같은 역할이라 보면 된다. </p>

<p><img src="http://latex.codecogs.com/gif.latex?min_%5Ctheta%20%5C%20C%20&amp;plus;%20%5Clambda%20B" alt=""></p>

<p>아주 작은 수의 <code>lambda</code> 를 사용하면 파라미터 <code>B</code> 가 커지는데, 이것은 <code>C</code> 가 커져 <code>A</code> 를 낮추고 <code>B</code> 를 높이는 것과 똑같다. 반대로 <code>C</code> 가 작으면 <code>A</code> 가 커지고, <code>B</code> 가 작아진다.</p>

<p>결국 <code>C</code> 를 쓰느냐 <code>lambda</code> 를 쓰느냐는, 어떤 항을 옵티마이제이션의 중심으로 두느냐다. 최적화된 파라미터를 찾는건 똑같다.</p>

<p>식을 마지막으로 정리하면,</p>

<p><img src="http://latex.codecogs.com/gif.latex?min_%5Ctheta%20%5C%20C%20%5C%20%5B%5Csum_%7Bi%3D1%7D%5Em%20y%5E%7B%28i%29%7Dcost_1%28%5Ctheta%5ETx%29%5C%20&amp;plus;%20%5C%20%281%20-%20y%5E%7B%28i%29%7D%29%5C%20%28cost_0%28%5Ctheta%5ETx%29%29%5D%5C%20&amp;plus;%20%5C%20%7B1%20%5Cover%202%7D%5Csum_%7Bj%3D1%7D%5En%20%5Ctheta_j%5E2" alt=""></p>

<p>결국 위 식 (<em>cost</em>) 를 최소화 하면, <code>y = 1</code> 일때 <code>0^Tx &gt;&gt; 0</code> 이 되므로 <code>h(x) == 1</code> 이란 뜻이다.</p>

<p><img src="http://img.my.csdn.net/uploads/201302/14/1360809865_3224.png" alt=""></p>

<p align="center">(<a href="http://blog.csdn.net/abcjennifer">http://blog.csdn.net/abcjennifer</a>)</p>

<h3 id="largeminginintuition">Large Mingin Intuition</h3>

<p><em>SVM</em> 은 <em>large margin classifier</em> 라 부르도 한다. 왜 그런게 한번 살펴보자.</p>

<p>두 집단을 구분하는 초록색, 자주색, 검은색 직선을 생각해 보자.</p>

<p><img src="http://img.my.csdn.net/uploads/201302/14/1360811170_6003.png" alt=""></p>

<p align="center">(<a href="http://blog.csdn.net/abcjennifer">http://blog.csdn.net/abcjennifer</a>)</p>

<p>검은색 선이 가장 낫고, 자주색과 초록색은 두 집단을 분리하긴 하는데 썩 만족할만하게는 아니다. 검은 선과 평행하고 각 점까지의 거리가 최소인 파란선을 그리자. 이걸 <em>margin</em> 이라 부른다. 다시 말해서 <em>margin</em> 이 클수록 좋은 <em>classification</em> 이다.</p>

<p><em>large margin</em> 하고 <em>SVM</em> 하고 무슨 상관일까? 그 전에 먼저 <code>C</code> 를 좀 살펴보자.</p>

<p><img src="http://img.my.csdn.net/uploads/201302/14/1360811018_1834.png" alt=""></p>

<p align="center">(<a href="http://blog.csdn.net/abcjennifer">http://blog.csdn.net/abcjennifer</a>)</p>

<p><code>z == 0^T x</code>, 의 범위를 생각해 보면 <code>y = 1</code> 일때 <code>z &gt;= 1</code> 이길 바란다. 반대로 <code>y = 0</code> 이면 <code>z &lt;= -1</code> 이면 <code>h(x)</code> 로 충분히 만족할 만한 값을 얻을 수 있다.</p>

<p>이 때 <code>C</code> 가 매우 크면 <code>A</code> 즉, 아래의 식은 굉장히 작아진다. 거의 0 에 가깝게</p>

<p><img src="http://latex.codecogs.com/gif.latex?%5Csum_%7Bi%3D1%7D%5Em%20y%5E%7B%28i%29%7Dcost_1%28%5Ctheta%5ETx%29%5C%20&amp;plus;%20%5C%20%281%20-%20y%5E%7B%28i%29%7D%29%5C%20%28cost_0%28%5Ctheta%5ETx%29%29" alt=""></p>

<p><img src="http://img.my.csdn.net/uploads/201302/14/1360811206_9816.png" alt=""></p>

<p align="center">(<a href="http://blog.csdn.net/abcjennifer">http://blog.csdn.net/abcjennifer</a>)</p>

<p>두 집단에 대해서 <code>C</code> 가 매우 크면, 다시 말해 <code>A</code> 가 <code>0</code> 에 가까우면 <em>overfitting</em> 된다 볼 수 있으므로 자주색과 비슷한 라인을 찾아낸다. 자주색 선은 모든 샘플에 대해 <em>large margin</em> 을 가지고 있지만 그렇게 썩 좋은 <em>classification</em> 이라 볼 수는 없다.</p>

<p>그러나 <code>C</code> 가 그렇게 크지 않으면 비 정상적인 샘플들은 조금 무시하고 검은색 선을 찾아낸다. 이게 <em>SVM</em> 이 작동하는 방식이다.</p>

<h3 id="mathematicsbehindlargemarginclassification">Mathematics Behind Large Margin Classification</h3>

<p><img src="http://img.my.csdn.net/uploads/201302/15/1360893984_1771.png" alt=""></p>

<p align="center">(<a href="http://blog.csdn.net/abcjennifer">http://blog.csdn.net/abcjennifer</a>)</p>

<p><img src="http://img.my.csdn.net/uploads/201302/15/1360893988_7434.png" alt=""></p>

<p align="center">(<a href="http://blog.csdn.net/abcjennifer">http://blog.csdn.net/abcjennifer</a>)</p>

<p>결국 <code>C</code> 가 아주 클 때 <code>A = 0</code> 이므로 <em>SVM</em> <em>cost fucntion</em> 을 최소화 하는 것은 아래 식과 동일하다. 그런데 이 식을 풀어 보면 </p>

<p><img src="http://latex.codecogs.com/gif.latex?min_%5Ctheta%20%5C%20%7B1%20%5Cover%202%7D%20%5Csum_%7Bj%3D1%7D%5En%20%5Ctheta_j%5E2%20%5C%5C%20%5C%5C%20%3D%20%7B1%20%5Cover%202%7D%20%5Cleft%20%5C%7C%20%5Ctheta%20%5Cright%20%5C%7C%5E2" alt=""></p>

<p>그리고 <code>0(theta)</code> 와 <code>x</code> 를 벡터이므로 <code>0^T x^(i) = p^(i) * ||0||</code> 라 볼 수 있다. (여기서 <code>p^(i)</code> 는 <code>x</code> 의 <code>0</code> 로의 <em>projection</em> 된 선의 길이)</p>

<p><img src="http://latex.codecogs.com/gif.latex?%5Ctheta%5ETx%20%5C%5C%20%5C%5C%20%3D%20p%5E%7B%28i%29%7D%20%5Cleft%20%5C%7C%20%5Ctheta%20%5Cright%20%5C%7C" alt=""></p>

<p>이제 이 식을 좀 활용해 보자. <code>C</code> 가 매우 클때는 <code>B</code> 만 최소화 하면 되는데</p>

<p><img src="http://latex.codecogs.com/gif.latex?min_%5Ctheta%20%5C%20%7B1%20%5Cover%202%7D%20%5Csum_%7Bj%3D1%7D%5En%20%5Ctheta_j%5E2" alt=""></p>

<p>이 식 자체가 <em>large margin</em> 을 찾아낸다. 왜 그런가 보면</p>

<p><img src="http://img.my.csdn.net/uploads/201302/15/1360893992_3213.png" alt=""></p>

<p align="center">(<a href="http://blog.csdn.net/abcjennifer">http://blog.csdn.net/abcjennifer</a>)</p>

<p>왼쪽 그래프의 계산 과정을 보면 <code>x1</code> 을 <code>0</code> 에 <em>projection</em> 해서 얻은 <code>p1</code> 이 매우 작다. 따라서 <code>p1 * ||0|| &gt;= 1</code> 에서 <code>||0||</code> 가 커야 전체 식이 1보다 커지는데, 이러면 식 <code>B</code> 를 최소화 할 수 없다. 마찬가지로 <code>p2</code> 는 매우 작은 음수고, <code>p2 * ||0|| &lt;= -1</code> 에서, <code>||0||</code> 가 매우 큰 음수여야 한다. 이 또한 <code>0</code> 를 크게 만드므로 식 <code>B</code> 가 작아지는 <code>0</code> 를 찾지 못한다. </p>

<p>결국 <code>p</code> 가 커야만 <code>0</code> 가 작아지기 때문에 <code>p</code> 를 크게 하는 <code>0</code> 만 찾고, 이것은 <em>large margin</em> 이다. 따라서 초록색 같은 <em>low margin</em> 의 <code>0</code> 는 선택되지 않는다. </p>

<p>정리하자면 <code>C</code> 가 매우 클때 <em>SVM</em> 은 <em>large magin</em> 을 찾고, 여기서 <code>C</code> 를 낮춤으로써 적당한 수준의 <em>classification</em> 을 얻을 수 있다.</p>

<p><img src="http://latex.codecogs.com/gif.latex?min_%5Ctheta%20%5C%20%7B1%20%5Cover%202%7D%20%5Csum_%7Bj%3D1%7D%5En%20%5Ctheta_j%5E2" alt=""></p>

<h3 id="kernels">Kernels</h3>

<p><img src="http://img.my.csdn.net/uploads/201302/15/1360895849_6087.png" alt=""></p>

<p align="center">(<a href="http://blog.csdn.net/abcjennifer">http://blog.csdn.net/abcjennifer</a>)</p>

<p><em>SVM</em> 으로 <em>non-linear decision boundary</em> 를 어떻게 찾아낼까? 단순히 <em>high polynomial features</em> 를 사용하는 것보다 더 나은 방법은 없을까? 고차 다항식은 이미지 처리 예제에서도 봤지만, 계산 비용이 너무 비싸다.</p>

<p><em>kernel</em> 이란 개념이 있다.</p>

<p><img src="http://img.my.csdn.net/uploads/201302/15/1360895854_4557.png" alt=""></p>

<p align="center">(<a href="http://blog.csdn.net/abcjennifer">http://blog.csdn.net/abcjennifer</a>)</p>

<p>수동으로 몇몇 <em>landmark</em> <code>l1, l2, ...</code> 을 고른후 이 <em>landmark</em> 사이와의 거리로 새로운 <em>feature</em> <code>f</code> 를 만든다.</p>

<p><img src="http://latex.codecogs.com/gif.latex?f_1%20%3D%20similarity%28x%2C%20l%5E%7B%281%29%7D%29%20%3D%20exp%20%28-%20%7B%7C%7Cx-l%5E%7B%281%29%7D%7C%7C%5E2%20%5Cover%202%5Csigma%5E2%7D%29" alt=""></p>

<p>dl <em>similarity function</em> 을 <em>kernel function</em> 특히 여기서 사용한 수식은 <em>gaussian kernel</em> 이라 부른다.</p>

<p><img src="http://img.my.csdn.net/uploads/201302/15/1360895859_5163.png" alt=""></p>

<p align="center">(<a href="http://blog.csdn.net/abcjennifer">http://blog.csdn.net/abcjennifer</a>)</p>

<p><code>x</code> 와 <code>l</code> 이 상당히 가까우면 <code>f</code> 는 <code>1</code> 에 근접하고, 상당히 멀면 <code>0</code> 에 가까워진다.</p>

<p><img src="http://img.my.csdn.net/uploads/201302/15/1360895862_8544.png" alt=""></p>

<p align="center">(<a href="http://blog.csdn.net/abcjennifer">http://blog.csdn.net/abcjennifer</a>)</p>

<p>위 그림은 시그마에 따른 <code>f</code> 값의 변화를 보여주는데, 시그마가 작으면 작을수록 조금만 멀어도 <code>f</code> 값은 <code>0</code> 에 가까워진다. </p>

<p><img src="http://img.my.csdn.net/uploads/201302/15/1360895867_6739.png" alt=""></p>

<p align="center">(<a href="http://blog.csdn.net/abcjennifer">http://blog.csdn.net/abcjennifer</a>)</p>

<p>데이터가 <em>landmark</em> 중 하나에 라도 가까우면 적어도 하나의 <code>f</code> 가 1이 되어, <code>h(x)</code> 가 1 이되고 반면 모든 <em>landmark</em> 에 멀면 모든 <code>f</code> 가 0 이 되어 <code>h(x)</code> 가 0 이된다.</p>

<p>그럼 이제, 문제는 어떻게 <em>landmark</em> 를 정할 것인가?</p>

<p><img src="http://img.my.csdn.net/uploads/201302/15/1360899128_1431.png" alt=""></p>

<p align="center">(<a href="http://blog.csdn.net/abcjennifer">http://blog.csdn.net/abcjennifer</a>)</p>

<p><img src="http://img.my.csdn.net/uploads/201302/15/1360899133_9301.png" alt=""></p>

<p align="center">(<a href="http://blog.csdn.net/abcjennifer">http://blog.csdn.net/abcjennifer</a>)</p>

<p><code>l1, ..., lm</code> 을 <code>x1, ..., xm</code> 라 하자. 즉 각 <em>training example</em> 이 <em>landmark</em> 가 된다. 이를 이용해 구한 <em>feature vector</em> <code>f^(i)</code> 중 하나는 <code>sim(x^i, l^i)</code> 이므로 1이 된다.</p>

<p><img src="http://img.my.csdn.net/uploads/201302/15/1360899136_2691.png" alt=""></p>

<p align="center">(<a href="http://blog.csdn.net/abcjennifer">http://blog.csdn.net/abcjennifer</a>)</p>

<p>따라서 주어진 <code>x</code> 에 대해 <code>m + 1</code> 의 벡터 <code>f</code> 를 구해 <code>0^Tf &gt;= 0</code> 이면 <code>y = 1</code> 이다. 그리고 이 때 <em>feature</em> 수가 <code>m</code> 이 되므로 </p>

<p><img src="http://latex.codecogs.com/gif.latex?min_%5Ctheta%20%5C%20C%20%5C%20%5Csum_%7By%3D1%7D%5Emcost_1%28%5Ctheta%5ETf%5E%7B%28i%29%7D%29%20&amp;plus;%20%281-y%5E%7B%28i%29%7D%29cost_0%28%5Ctheta%5ETf%5E%7B%28i%29%7D%29%29%20&amp;plus;%20%7B1%20%5Cover%202%7D%20%5Csum_%7Bj%3D1%7D%5Em%5Ctheta_j%5E2" alt=""></p>

<p>마지막 항을 좀 자세히 보면</p>

<p><img src="http://latex.codecogs.com/gif.latex?%5Csum_%7Bj%3D1%7D%5Ctheta_j%5E2%20%5C%5C%20%5C%5C%20%3D%20%5Ctheta%5ET%20%5Ctheta" alt=""></p>

<p>인데 <em>SVM</em> 실제 구현에서는 가운데 <code>M</code> 매트릭스를 삽입해 좀더 효율적으로 돌아가도록 한다. 이 <code>M</code> 은 어떤 <em>kernel</em> 을 사용하는지에 따라 다르다.</p>

<p><img src="http://latex.codecogs.com/gif.latex?%5Ctheta%5ET%20M%20%5C%20%5Ctheta" alt=""></p>

<p><em>logistic regression</em> 에 <em>kernel</em> 을 사용할 수도 있겠지만, 상당히 느리다. 반면 <em>SVM</em> 에서는 마지막 항을 위 처럼 수정할 수 있기에 빠르게 동작한다.</p>

<h3 id="biasvsvarianceinsvm">Bias vs Variance in SVM</h3>

<p><img src="http://img.my.csdn.net/uploads/201302/15/1360899140_2255.png" alt=""></p>

<p align="center">(<a href="http://blog.csdn.net/abcjennifer">http://blog.csdn.net/abcjennifer</a>)</p>

<p>(1) <code>C</code> 가 크면 <em>low bias</em>, <em>high variance</em>  (== <em>small <code>lambda</code></em>) <br>
(2) <code>C</code> 가 작으면 <em>high bias</em>, <em>low variance</em>  (== <em>large <code>lambda</code></em>)  </p>

<p><code>sigma</code> 가 크면 <code>f</code> 가 적게 변하기 때문에 인풋 <code>x</code> 에 대해서도 <em>high bias</em>, <em>low variance</em> 다.</p>

<h3 id="usingansvm">Using an SVM</h3>

<p><img src="http://img.my.csdn.net/uploads/201302/15/1360901245_9359.png" alt=""></p>

<p align="center">(<a href="http://blog.csdn.net/abcjennifer">http://blog.csdn.net/abcjennifer</a>)</p>

<p>라이브러리를 사용하더라도 <code>C</code> 와 어떤 <em>kernel</em> 을 사용할지는 골라야 한다.</p>

<p><em>feature</em> 가 크고, 트레이닝셋이 작을때는 <em>overfitting</em> 될 수 있으므로 <em>linear kernel</em> 을 사용하는 편이 낫다.</p>

<p>반면 <code>n</code> 이 작고, <code>m</code> 이 클 경우에는 <em>non-linear</em> 가설일 수 있으므로 <em>gaussian kernel</em> 을 사용할 수 있다. 그러면 <code>sigma</code> 를 골라야 한다. </p>

<p><img src="http://img.my.csdn.net/uploads/201302/15/1360901242_6422.png" alt=""></p>

<p align="center">(<a href="http://blog.csdn.net/abcjennifer">http://blog.csdn.net/abcjennifer</a>)</p>

<p><em>SVM</em> 라이브러리를 이용할때는 <code>kernel function</code> 을 직접 구현해야 한다. 이걸 이용해서 라이브러리는 <code>x</code> 에 대해 <code>f1, ..., fl</code> 을 계산한다.</p>

<p>만약에 <em>feature</em> 의 스케일이 다르면, <code>x1 = 10000, x2 = 5, ...</code> <code>||x-l||^2</code> 값이 숫자가 큰 항에 의해 좌우될 수 있으므로 <em>feature scailing</em> 을 하는편이 좋다.</p>

<h4 id="otherchoicesofkernel">Other choices of kernel</h4>

<p><img src="http://img.my.csdn.net/uploads/201302/15/1360901245_9359.png" alt=""></p>

<p align="center">(<a href="http://blog.csdn.net/abcjennifer">http://blog.csdn.net/abcjennifer</a>)</p>

<p><em>SVM</em> 구현들이 계산을 최적화 하기위해 다양한 트릭을 이용한다. 이로 인해 모든 <em>similarity function</em> 유효한 커널이 되는건 아니고, <em>"Mercer's Theorem"</em> 을 만족해야만 한다. <del>인용하려 했는데 무슨말인지 모르겠음</del></p>

<p>그렇다고 커널이 <em>linear</em> 와 <em>gaussian</em> 만 있는건 아니고 다양한 커널이 있다. 그림을 참조하자.</p>

<h4 id="multiclassclassification">Multi-class classification</h4>

<p><img src="http://img.my.csdn.net/uploads/201302/15/1360901253_5022.png" alt=""></p>

<p align="center">(<a href="http://blog.csdn.net/abcjennifer">http://blog.csdn.net/abcjennifer</a>)</p>

<p>대부분의 <em>SVM</em> 라이브러리들은 <em>multi-class</em> 에 대한 함수를 제공한다. 그러나 이것들을 사용하는 대신 <em>one-vs-all</em> 방법을 사용할 수도 있다. <code>k</code> 개의 클래스가 있다면 <code>k</code> 개의 <em>SVM</em> 훈련시키면 된다.</p>

<h4 id="logisticregressionvssvm">Logistic regression vs SVM</h4>

<p><img src="http://my.csdn.net/uploads/201208/12/1344759226_6088.png" alt=""></p>

<p align="center">(<a href="http://blog.csdn.net/abcjennifer">http://blog.csdn.net/abcjennifer</a>)</p>

<p>(1) <code>n &gt;= m</code> 이면 <em>logistic regression</em> 이나 <em>linear kernel</em> 이 낫다. <br>
(2) <code>n</code> 이 작고, <code>m</code> 이 중간 사이즈면 <em>gaussian kernel</em> 을 <br>
(3) <code>n</code> 이 작고 <code>m</code> 이 크면 <em>gaussian</em> 은 상당히 느려진다. <em>feature</em> 를 좀 수정하고, <em>logistic</em> 이나 <em>linear kernel</em> 을 이용한다.</p>

<p><em>SVM</em> 의 장점은 다양한 <em>kernel</em> 을 <em>non-linear function</em> 을 훈련시키기 위해 사용할 수 있다는 점이다.</p>

<h3 id="references">References</h3>

<p>(1) <em>Machine Learning</em> by <strong>Andrew NG</strong> <br>
(2) <a href="http://blog.csdn.net/linuxcumt">http://blog.csdn.net/linuxcumt</a> <br>
(3) <a href="http://blog.csdn.net/abcjennifer">http://blog.csdn.net/abcjennifer</a>  </p>
    </section>

    <footer>
      
      <section class="author_info margin_top_big">
        <div class="alignleft border rad_circle" style="height: 87px; width: 87px; background-image: url(http://www.gravatar.com/avatar/aa2032ba2302419e3c2ede54f1fbf687?d=404&amp;s=250); background-size: cover;"></div>
        <p class="margin_left_medium text small">Author</p>
        <p class="margin_left_medium text bold"><a href="http://language.is">1ambda</a></p>
        <p class="margin_left_medium text small">Lisp, Emacs, FP</p>
      </section>
      
    </footer>

    

    
    <div id="disqus_thread" class="margin_top_big"></div>
<script type="text/javascript">
  /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
  var disqus_shortname = '1ambda'; // required: replace example with your forum shortname
  var disqus_identifier = '64';
  var disqus_url = 'http://1ambda.github.io/machine-learning-week-7/';

  /* * * DON'T EDIT BELOW THIS LINE * * */
  (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>

    </article>
</main>


  
  <script src="../assets/fitvids/jquery.fitvids.js"></script>
<script>
$(document).ready(function(){
  // Target your .container, .wrapper, .post, etc.
  $("section").fitVids();
});
</script>


  <footer class="blog_info margin_top_big padding_medium text center">
    <h5 class="text book small">© 2014 <a href="..">Old Lisper</a>. All rights reserved.</h5>
    <h5 class="text book small"><a href="https://github.com/dreyacosta/velox" target="_blank" class="text bold">Velox theme</a> by <a href="http://dreyacosta.com/">David Rey</a></h5>
    <h5 class="text book small">Proudly published with <a href="http://ghost.org"><span>Ghost</span></a></h5>

  </footer>

  
  <script type="text/javascript">
/* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
var disqus_shortname = '1ambda'; // required: replace example with your forum shortname

/* * * DON'T EDIT BELOW THIS LINE * * */
(function () {
 var s = document.createElement('script'); s.async = true;
 s.type = 'text/javascript';
 s.src = '//' + disqus_shortname + '.disqus.com/count.js';
 (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
 }());
</script>



  </body>
  