<!DOCTYPE html>
<html itemscope itemtype="http://schema.org/Blog">
<head>
  <meta http-equiv="Content-Type" content="text/html" charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />

  <title>Machine Learning, Week 10</title>
  <meta name="description" content="" />

  <meta name="HandheldFriendly" content="True" />
  <meta name="MobileOptimized" content="320" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />

  

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Machine Learning, Week 10">
  <meta name="twitter:description" content="이번 주에는 mini-batch, stochastic graident descent, online learning, map-reduce 등의 개념에 대해 배운다. Learning With Large Datasets (http://blog.csdn.net/linuxcumt) 왜 그렇게 큰 데이터 셋이 필요할까? 좋은 퍼포먼스를 얻기 위한 한 가지 방법이, low bias 알고리즘에 massive data 를 활용해 훈련하는 것이기 때문이다. (http://blog.csdn.net/linuxcumt">
  <meta name="twitter:creator" content="@yourTwitterUsername">
  <meta name="twitter:image" content="">
  <meta name="twitter:url" content="http://1ambda.github.io/machine-learning-week-10/">
  <meta name="twitter:domain" content="http://1ambda.github.io">

  

  <link rel="author" href="https://plus.google.com/101105410053351451441?rel=author" />

  <link rel="shortcut icon" href="../favicon.ico">

  <link rel="stylesheet" type="text/css" href="http://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" />
  <link rel="stylesheet" type="text/css" href="http://fonts.googleapis.com/css?family=Droid+Serif" />
  <link rel='stylesheet' type='text/css' href="http://fonts.googleapis.com/css?family=Open+Sans:600,300" />
  <link rel="stylesheet" type="text/css" href="../assets/stylesheets/xpressio.css" />
  <link rel="stylesheet" type="text/css" href="../assets/1ambda/1ambda.css" />
  <script type="text/javascript" src="../assets/1ambda/modernizr.js">
  </script>
  <script type="text/javascript" src="../assets/1ambda/detectizr.min.js">
  </script>

  <!--load css if windows -->
  <script type="text/javascript">
    if (Modernizr.windows) {
      file = location.pathname.split( "/" ).pop();
      link = document.createElement( "link" );
      link.href = "/assets/1ambda/1ambda_windows.css";
      link.type = "text/css";
      link.rel = "stylesheet";
      link.media = "screen,print";
      document.getElementsByTagName("head")[0].appendChild( link );
    }
  </script>


  
  <link rel="stylesheet" href="../assets/highlight/styles/github.css">
<script src="../assets/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>


  <meta name="generator" content="Ghost 0.5" />
<link rel="alternate" type="application/rss+xml" title="Old Lisper" href="../rss">
<link rel="canonical" href="http://1ambda.github.io/machine-learning-week-10/" />

  
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-52181619-1', '1ambda.github.io');
  ga('send', 'pageview');
</script>

  
</head>
<body>

  
  <script src="../public/jquery.js?v=94da418e4f"></script>

  

<header class="site_width text center padding_top_big margin_bottom_big">
  
  <h1 class="blog_title margin_bottom_small"><a href="http://1ambda.github.io">Old Lisper</a></h1>
  <h4 class="text book">Lisp, Emacs, Scala</h4>
  
  <div class="social border solid top_small bottom_small padding_medium">
  <h6 class="text book color c_black_medium without_margin padding_right_big"><a href="/about-me"><i class="fa fa-user"></i> <span class="margin_left_small desktop">About me</span></a></h6>
  <h6 class="text book color c_black_medium without_margin padding_right_big"><a href="http://kr.linkedin.com/in/1ambda" target="_blank"><i class="fa fa-linkedin-square"></i> <span class="margin_left_small desktop">Linkedin</span></a></h6>
  <h6 class="text book color c_black_medium without_margin"><a href="http://github.com/1ambda" target="_blank"><i class="fa fa-github"></i> <span class="margin_left_small desktop">GitHub</span></a></h6>
</div>

</header>

<main class="site_width" role="main">
  <article class="post tag-coursera tag-machine-learning tag-stochastic-gradient-descent tag-mini-batch-gradient-descent tag-online-learning">

    

    <header class="text center margin_bottom_medium">
      <h5 class="text book small uppercase color c_black_light margin_bottom_small">Posted in <a href="../tag/coursera">coursera</a>, <a href="../tag/machine-learning">machine learning</a>, <a href="../tag/stochastic-gradient-descent">stochastic gradient descent</a>, <a href="../tag/mini-batch-gradient-descent">mini-batch gradient descent</a>, <a href="../tag/online-learning">online learning</a></h5>
      <h1 class="margin_bottom_medium">Machine Learning, Week 10</h1>
      <h5 class="text book small uppercase color c_black_light margin_bottom_small"><time datetime="2014-12-15">Monday, December 15, 2014</time>
      <br/><br/>
       <a href="http://1ambda.github.io/machine-learning-week-10/#disqus_thread">Comment</a>
      </h5>
    </header>

    <section>
      <p>이번 주에는 <em>mini-batch, stochastic graident descent</em>, <em>online learning</em>, <em>map-reduce</em> 등의 개념에 대해 배운다.</p>

<h3 id="learningwithlargedatasets">Learning With Large Datasets</h3>

<p><img src="http://img.my.csdn.net/uploads/201302/22/1361499744_2717.jpg" alt="" /></p>

<p align="center">(<a href='http://blog.csdn.net/linuxcumt'>http://blog.csdn.net/linuxcumt</a>)</p>

<p>왜 그렇게 큰 데이터 셋이 필요할까? 좋은 퍼포먼스를 얻기 위한 한 가지 방법이, <em>low bias</em> 알고리즘에 <em>massive data</em> 를 활용해 훈련하는 것이기 때문이다.</p>

<p><img src="http://img.my.csdn.net/uploads/201302/22/1361499747_9327.jpg" alt="" /></p>

<p align="center">(<a href='http://blog.csdn.net/linuxcumt'>http://blog.csdn.net/linuxcumt</a>)</p>

<p>그러나 커다란 데이터 셋을 연산하기 위해서는 연산비용이 정말 비싸다. 예를 들어 
<code>m = 100,000,000</code> 이라 하면 <em>gradient</em> 를 계산하기 위해 매번 <code>k * m</code> 의 연산이 필요하다.</p>

<p>따라서 모든 데이터를 이용해 알고리즘을 훈련하기 보다는, 랜덤하게 고른 작은 서브셋에 대해서 알고리즘을 개발하고, 이후에 전체 데이터에 대해서 훈련하는 방법을 쓰기도 한다.</p>

<p>그러면 <code>m</code> 이 작아도 알고리즘이 충분히 잘 훈련된다는 것을 어떻게 보장할까? 이는 <em>learning curve</em> 를 그려보면 된다.</p>

<p>위 슬라이드에서 우측 하단은 <em>high bias</em> 알고리즘인데 <code>m</code> 을 많이 투입해도 별다른 이득이 없으므로 적당한 수준에서 <code>m</code> 을 정하면 된다.</p>

<p>물론 만든 알고리즘이 우측처럼 <em>high bias</em> 로 나오면, 좀 더 자연스러운 생각은 <em>hidden layer</em> 를 추가한다거나, 새로운 <em>feature</em> 를 도입해서 <em>bias</em> 를 낮추는 것이다.</p>

<p><br/></p>

<h3 id="stochsticgradientdescent">Stochstic Gradient Descent</h3>

<p><img src="http://img.my.csdn.net/uploads/201302/22/1361500908_4667.jpg" alt="" /></p>

<p align="center">(<a href='http://blog.csdn.net/linuxcumt'>http://blog.csdn.net/linuxcumt</a>)</p>

<p><em>gradient descent</em> 를 이용하는 <em>linear regression</em> 에서</p>

<p><img src="http://latex.codecogs.com/gif.latex?h_%5Ctheta%28x%29%20%3D%20%5Csum_%7Bj%3D0%7D%5En%20%5C%20%5Ctheta_jx_j" alt="" /></p>

<p><img src="http://latex.codecogs.com/gif.latex?J_%7Btrain%7D%28%5Ctheta%29%20%3D%20%7B1%20%5Cover%202m%7D%20%5Csum_%7Bi%3D1%7D%5Em%5C%20%28h_%5Ctheta%28x%29%5E%7B%28i%29%7D%20-%20y%5E%7B%28i%29%7D%29%5E2" alt="" /></p>

<p>이미 언급했듯이 <em>batch gradient descent</em> 의 문제는, <code>m</code> 이 크면 <code>J</code> 의 연산이 어마어마하게 많아진다는 것이다. 매 스텝마다 <code>m</code> 을 읽고, 계산값을 저장하기 때문이다.</p>

<p><img src="http://img.my.csdn.net/uploads/201302/22/1361501694_2445.jpg" alt="" /></p>

<p align="center">(<a href='http://blog.csdn.net/linuxcumt'>http://blog.csdn.net/linuxcumt</a>)</p>

<p>이 문제를 해결하기 위해 <em>stochastic gradient descent</em> 에서는 한 턴에 한 쌍의 <code>x, y</code> 에 대해서만 <em>gradient</em> 를 계산한다.</p>

<p><em>batch gradient decsent</em> 에서는 한 턴마다 모든 모든 쌍의 <code>x, y</code> 에 대해     <em>gradient</em> (= <code>theta_j</code>) 를 계산 했었다.</p>

<p><img src="http://latex.codecogs.com/gif.latex?cost%28%5Ctheta%2C%20%28x%5E%7B%28i%29%7D%2C%20y%5E%7B%28i%29%7D%29%29%20%3D%20%7B1%20%5Cover%202%7D%5C%20%28h_%5Ctheta%28x%5E%7B%28i%29%7D%20-%20y%5E%7B%28i%29%7D%29%5E2" alt="" /></p>

<p><img src="http://latex.codecogs.com/gif.latex?J_%7Btrain%7D%28%5Ctheta%29%20%3D%20%7B1%20%5Cover%20m%7D%20%5Csum_%7Bi%3D1%7D%5Em%20cost%28%5Ctheta%2C%20%28x%5E%7B%28i%29%7D%2C%20y%5E%7B%28i%29%7D%29%29" alt="" /></p>

<p>라고 하면</p>

<pre><code>- Randomly shuffle dataset  
- Repeat for i = 1 to m
  - 0_j := 0_j - a * derivative of cost(0, (xi, yi)
</code></pre>

<p>즉 <code>J_train</code> 의 미분 대신에 <code>cost</code> 의 미분값을 이용해서 연산을 줄이는 방법이다. 이 때 데이터가 이미 랜덤하게 섞였다는 점을 기억하자. 기하학적으로 보면</p>

<p><img src="http://img.my.csdn.net/uploads/201302/22/1361502040_9252.jpg" alt="" /></p>

<p align="center">(<a href='http://blog.csdn.net/linuxcumt'>http://blog.csdn.net/linuxcumt</a>)</p>

<p><em>batch</em> 에서는 올바른 방향을 향해 가지만, 보폭이 좀 좁다. <em>stochastic</em> 은 이리 갔다, 저리 갔다 하지만 결국에는 최저점을 향해 간다. 다만 <em>global optima</em> 에 도달하지 못하고 그 근처에 도착할 수 있다.</p>

<p><code>m</code> 이 굉장히 크면, <em>repeat</em> 부분의 루프를 1번만 돌려도 될 테지만, 작으면 여러번 돌려서 최대한 좋은 퍼포먼스를 내도록 훈련시킬 수 있다.</p>

<p><br/></p>

<h3 id="minbatchgradientdescent">Min-Batch Gradient Descent</h3>

<p><img src="http://img.my.csdn.net/uploads/201302/22/1361503946_9357.jpg" alt="" /></p>

<p align="center">(<a href='http://blog.csdn.net/linuxcumt'>http://blog.csdn.net/linuxcumt</a>)</p>

<ul>
<li><strong>Batch gradient descent:</strong> Use all <code>m</code> examples in each iteration</li>
<li><strong>Stochastic gradient descent:</strong> Use <code>1</code> example in each iteration</li>
<li><strong>Batch gradient descent:</strong> Use <code>b</code> examples in each iteration</li>
</ul>

<p><img src="http://latex.codecogs.com/gif.latex?%5Ctheta_j%20%3A%3D%20%5Ctheta-j%20-%20%7B%5Calpha%20%5Cover%20b%7D%20%5Csum_%7Bk%20%3D%20i%7D%5E%7Bi%20&plus;%20b%20-%201%7D%20%28h_%5Ctheta%28x%5E%7B%28k%29%7D%29%20-%20y%5E%7B%28k%29%7D%29%20x_j%5E%7B%28k%29%7D" alt="" /></p>

<p><img src="http://img.my.csdn.net/uploads/201302/22/1361504164_4561.jpg" alt="" /></p>

<p align="center">(<a href='http://blog.csdn.net/linuxcumt'>http://blog.csdn.net/linuxcumt</a>)</p>

<p><code>b</code> 는 보통 <code>2 - 100</code> 사이의 값이기 때문에 <em>batch</em> 보다 훨씬 빠르다. 또한 <em>vectorization</em> 을 이용하면 <em>gradient computation</em> 을 <em>partially parallelize</em> 할 수 있기 때문에 <em>stochastic</em> 보다 더 빠를 수 있다. <del>병렬화의 미덕</del></p>

<p>단점으로는 추가적인 파라미터 <code>b</code> 가 필요하다는 점이다. 그러나 <em>vectorization</em> 을 잘 이용하면 더 빨라지므로 문제 없다.</p>

<p><br/></p>

<h3 id="stochasticgradientdescentconvergence">Stochastic Gradient Descent Convergence</h3>

<p><img src="http://img.my.csdn.net/uploads/201302/22/1361519060_5993.jpg" alt="" /></p>

<p align="center">(<a href='http://blog.csdn.net/linuxcumt'>http://blog.csdn.net/linuxcumt</a>)</p>

<p><em>convergence</em> 를 검증하는 방법으로, 훈련시키는 동안 얻은 <code>cost(0, (xi yi)</code> 평균값을 이용해 플롯을 그릴 수 있다.     </p>

<p><img src="http://img.my.csdn.net/uploads/201302/22/1361519096_5186.jpg" alt="" /></p>

<p align="center">(<a href='http://blog.csdn.net/linuxcumt'>http://blog.csdn.net/linuxcumt</a>)</p>

<p><em>stochastic</em> 은 <em>global optimum</em> 을 찾아내지 못할 수도 있기 때문에, 그 주변에서 알짱거릴 수도 있다.</p>

<p>더 많은 <code>m</code> 을 투입하면, 까끌거리는 선보다 좀 매끄러운 곡선을 얻을 수도 있다.</p>

<p>때때로 알고리즘이 전혀 학습하지 못하는 것 처럼 보일수도 있는데, 그럴 경우 <code>m</code> 을 더 투입하면 좀 경사가 낮은 커브로 조금씩 <em>decreasing</em> 할 수 있다. 이를 보면 결국 훈련되긴 하는데, 평균값을 플랏으로 그리니 들쭉날쭉 해 보이는 것이다. (물론 학습하지 못하는 경우도 있다. <code>m</code> 을 더 늘려서 확인해 보자.)</p>

<p><code>cost</code> 값이 증가한다면 더 작은 <em>learning late</em> 값을 이용하자.</p>

<p><img src="http://img.my.csdn.net/uploads/201302/22/1361519184_7199.jpg" alt="" /></p>

<p align="center">(<a href='http://blog.csdn.net/linuxcumt'>http://blog.csdn.net/linuxcumt</a>)</p>

<p><em>learning rate</em> 와 관련해서 위 슬라이드처럼 식을 만들면, 이터레이션 넘버가 천천히 증가하면서 <code>alpha</code> 가 감소해 <em>converge</em> 하는 결과를 얻을 수 있다.</p>

<blockquote>
  <p>If we reduce learning rate <code>alpha</code> (and run stochastic gradient descent long enough), it's possible that we may find a set of better parameters than with large <code>alpha</code></p>
</blockquote>

<p><br/></p>

<h3 id="onlinelearning">Online Learning</h3>

<p><img src="http://img.my.csdn.net/uploads/201302/22/1361520551_7175.jpg" alt="" /></p>

<p><em>online learning</em> 에서는 데이터를 얻어 <code>theta</code> 를 업데이트하는데 사용하고, 버린다. 큰 사이트라면 데이터가 지속적으로 들어오기 때문에, <em>trianing data</em> 를 볼 필요가 없다. 다시 말해 같은 데이터를 두번 이상 쓰지 않는다는 말이다.</p>

<p>또 다른 장점으로는 사용자의 취향 변화를 빠르게 반영할 수 있다는 점이다.</p>

<blockquote>
  <p>Can adopt to changing user preference</p>
</blockquote>

<p><img src="http://img.my.csdn.net/uploads/201302/22/1361520594_9988.jpg" alt="" /></p>

<p align="center">(<a href='http://blog.csdn.net/linuxcumt'>http://blog.csdn.net/linuxcumt</a>)</p>

<p><em>product search</em> 에 <em>predicted CTR</em> 를 이용해, 검색어와 잘 매칭되는 상품을 결과로 돌려수 있다. 이때 매 검색마다 돌려주는 검색결과는 일종의 <em>training set</em> 이 된다.</p>

<ul>
<li>special offers</li>
<li>customized selection</li>
</ul>

<p>등에도 사용할 수 있다.</p>

<p><br/></p>

<h3 id="mapreduceanddataparallelism">Map Reduce and Data Parallelism</h3>

<p>데이터가 어마어마하게 많으면, 하나의 컴퓨터에서 머신러닝 알고리즘을 돌리기가 좀 힘들다. 어떻게 해결할까?</p>

<p><img src="http://img.my.csdn.net/uploads/201302/22/1361522176_1942.jpg" alt="" />
<img src="http://img.my.csdn.net/uploads/201302/22/1361522180_7521.jpg" alt="" /></p>

<p>쉽게 말하면, 분산해서 처리할 수 있는 결과는 <code>map</code> 으로 해결하고, 이 결과들을 이용해 전체적인 결과는 <code>reduce</code> 가 계산한다. (실제로는 <code>reduce</code> 도 여러개 일 수 있다)</p>

<p><img src="http://img.my.csdn.net/uploads/201302/22/1361522184_5033.jpg" alt="" /></p>

<p align="center">(<a href='http://blog.csdn.net/linuxcumt'>http://blog.csdn.net/linuxcumt</a>)</p>

<blockquote>
  <p>Many lenaring algorithms can be expressed as computing sums of functions over the training set</p>
</blockquote>

<p>이렇기 때문에 <em>map-reduce</em> 가 큰 데이터셋에 대한 계산 처리 방법으로 좋은 해결책이 될 수 있다.</p>

<p><img src="http://img.my.csdn.net/uploads/201302/22/1361522188_9650.jpg" alt="" /></p>

<p align="center">(<a href='http://blog.csdn.net/linuxcumt'>http://blog.csdn.net/linuxcumt</a>)</p>

<p>요즘엔 대부분의 프로세서가 멀티코어이기 때문에, 하나의 컴퓨터에서도 병렬화를 이용해 계산을 빠르게 해 낼 수 있다. 이 경우는 <em>network latency</em> 등에 대해 생각을 안해도 된다. 참고로 좋은 라이브러리들은 자동으로 연산을 병렬화한다. </p>

<p><br/></p>

<h3 id="photoocrandpipeline">Photo OCR and Pipeline</h3>

<p>머신러닝 예제로 <em>Photo OCR</em> 을 알아보자.</p>

<p><img src="http://img.my.csdn.net/uploads/201302/27/1361935712_3407.jpg" alt="" /></p>

<p align="center">(<a href='http://blog.csdn.net/linuxcumt'>http://blog.csdn.net/linuxcumt</a>)</p>

<p><em>Photo OCR pipeline</em> 은 </p>

<ul>
<li>Image</li>
<li>Text detection</li>
<li>Character segmentation</li>
<li>Character recognition</li>
</ul>

<p>의 단계를 거친다. 각 단계마다 머신러닝을 적용할 수 있다.</p>

<p><br/></p>

<h3 id="slidingwindows">Sliding Windows</h3>

<p><img src="http://img.my.csdn.net/uploads/201302/27/1361936303_5994.jpg" alt="" />
<img src="http://img.my.csdn.net/uploads/201302/27/1361936307_1350.jpg" alt="" />
<img src="http://img.my.csdn.net/uploads/201302/27/1361937647_2708.jpg" alt="" /></p>

<p>텍스트나, 보행자등 특정 패턴을 검색하기 위해 이동하는 <em>rectangle</em> 의 단위를 <em>step-size, slide</em> 라 부른다. <em>slide</em> 의 사이즈를 변경해 가면서 패턴을 파악하는 방법을 <em>sliding window</em> 라 부른다.</p>

<p><img src="http://img.my.csdn.net/uploads/201302/27/1361937654_3410.jpg" alt="" />
<img src="http://img.my.csdn.net/uploads/201302/27/1361937664_2466.jpg" alt="" /></p>

<p align="center">(<a href='http://blog.csdn.net/linuxcumt'>http://blog.csdn.net/linuxcumt</a>)</p>

<p>텍스트를 인식해서, 근처의 텍스트와 묶는 <em>expansion</em> 작업을 하고 <em>character segmentation</em> 단계로 넘어간다. </p>

<p><img src="http://img.my.csdn.net/uploads/201302/27/1361937671_6657.jpg" alt="" />
<img src="http://img.my.csdn.net/uploads/201302/27/1361937676_4874.jpg" alt="" /></p>

<p align="center">(<a href='http://blog.csdn.net/linuxcumt'>http://blog.csdn.net/linuxcumt</a>)</p>

<p><br/></p>

<h3 id="artificialdata">Artificial Data</h3>

<p><em>low bias</em> 에 <em>massive data set</em> 을 조합하면 좋은 퍼포먼스가 나오긴 하는데, 어디서 커다란 데이터셋을 구할까? 작은 데이터 셋으로 커다란 데이터셋을 인위적으로 만들 수 있을까?</p>

<p><img src="http://img.my.csdn.net/uploads/201302/27/1361948453_3756.jpg" alt="" />
<img src="http://img.my.csdn.net/uploads/201302/27/1361948457_9107.jpg" alt="" />
<img src="http://img.my.csdn.net/uploads/201302/27/1361948462_1113.jpg" alt="" /></p>

<p align="center">(<a href='http://blog.csdn.net/linuxcumt'>http://blog.csdn.net/linuxcumt</a>)</p>

<p>스케일링, 로테이션, 디스토션, 백그라운드 수정 등 다양한 조합을 통해 진짜처럼 보이는 <em>synthetic data</em> 를 얻을 수 있다. 마찬가지로, <em>speech recognition</em> 에도 <em>synthetic data</em> 를 만들어 퍼포먼스를 높일 수 있다.</p>

<p><img src="http://img.my.csdn.net/uploads/201302/27/1361948466_5032.jpg" alt="" /></p>

<ul>
<li><em>synthetic data</em> 를 만들기 전에 <em>low bias classifier</em> 인지 확인하자.</li>
<li>데이터를 조합하는데 들어가는 노력이 얼마나 들까 생각해보자</li>
<li><em>crowd source</em> 를 고려하자. (e.g. Amazon Mechanical Turk)</li>
</ul>

<p>10 초당 1개의 <em>example</em> 을 수동으로 얻는다면, 10000 개를 얻는데 대략 3.5일의 시간이 걸린다.</p>

<h3 id="ceilinganalysis">Ceiling Analysis</h3>

<p>이전의 <em>Photo OCR</em> 문제에서 퍼포먼스를 높이려면 파이프라인의 각 단계 중 어느 부분에 가장 많은 노력을 들여야할까? </p>

<p><img src="http://img.my.csdn.net/uploads/201302/27/1361952527_2269.jpg" alt="" /></p>

<p align="center">(<a href='http://blog.csdn.net/linuxcumt'>http://blog.csdn.net/linuxcumt</a>)</p>

<p>각 단계에서 수동으로 정확도 100% 를 만들었을때와, 전체적인 정확도를 비교해서 어느 부분을 향상 시켰을때 가장 효율적일지를 파악할수 있다.  </p>

<p><img src="http://img.my.csdn.net/uploads/201302/27/1361952535_9451.jpg" alt="" />
<img src="http://img.my.csdn.net/uploads/201302/27/1361952540_1528.jpg" alt="" /></p>

<p align="center">(<a href='http://blog.csdn.net/linuxcumt'>http://blog.csdn.net/linuxcumt</a>)</p>

<p><br/></p>

<h3 id="summary">Summary</h3>

<p><img src="http://img.my.csdn.net/uploads/201302/27/1361952938_3624.jpg" alt="" /></p>

<p align="center">(<a href='http://blog.csdn.net/linuxcumt'>http://blog.csdn.net/linuxcumt</a>)</p>

<p><strong>supervised learning</strong> 의 종류로</p>

<ul>
<li>linear regresison</li>
<li>logistic regression</li>
<li>neural networks</li>
<li>SVM</li>
</ul>

<p><strong>unsupervised learning</strong> 으로</p>

<ul>
<li>k-means</li>
<li>PCA</li>
<li>anomaly detection</li>
</ul>

<p>또한 머신 러닝의 응용으로</p>

<ul>
<li>recommender system</li>
<li>large scale ML</li>
</ul>

<p>마지막으로 머신러닝에 도움이 되는 주제로</p>

<ul>
<li>bias vs variance</li>
<li>regularization</li>
<li>evaluation technique</li>
<li>learning curve</li>
<li>error analysis</li>
<li>ceiling analysis</li>
</ul>

<p>등을 배웠다.</p>

<p><img src="http://img.my.csdn.net/uploads/201302/27/1361953290_6926.jpg" alt="" /></p>

<h3 id="references">References</h3>

<p>(1) <em>Machine Learning</em> by <strong>Andrew NG</strong> <br />
(2) <a href="http://blog.csdn.net/linuxcumt">http://blog.csdn.net/linuxcumt</a> <br />
(3) <a href="http://blog.csdn.net/abcjennifer">http://blog.csdn.net/abcjennifer</a>  </p>
    </section>

    <footer>
      
      <section class="author_info margin_top_big">
        <div class="alignleft border rad_circle" style="height: 87px; width: 87px; background-image: url(http://www.gravatar.com/avatar/aa2032ba2302419e3c2ede54f1fbf687?d=404&amp;s=250); background-size: cover;"></div>
        <p class="margin_left_medium text small">Author</p>
        <p class="margin_left_medium text bold"><a href="http://language.is">1ambda</a></p>
        <p class="margin_left_medium text small">Lisp, Emacs, FP</p>
      </section>
      
    </footer>

    

    
    <div id="disqus_thread" class="margin_top_big"></div>
<script type="text/javascript">
  /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
  var disqus_shortname = '1ambda'; // required: replace example with your forum shortname
  var disqus_identifier = '80';
  var disqus_url = 'http://1ambda.github.io/machine-learning-week-10/';

  /* * * DON'T EDIT BELOW THIS LINE * * */
  (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>

    </article>
</main>


  
  <script src="../assets/fitvids/jquery.fitvids.js"></script>
<script>
$(document).ready(function(){
  // Target your .container, .wrapper, .post, etc.
  $("section").fitVids();
});
</script>


  <footer class="blog_info margin_top_big padding_medium text center">
    <h5 class="text book small">&copy; 2014 <a href="..">Old Lisper</a>. All rights reserved.</h5>
    <h5 class="text book small"><a href="https://github.com/dreyacosta/velox" target="_blank" class="text bold">Velox theme</a> by <a href="http://dreyacosta.com/">David Rey</a></h5>
    <h5 class="text book small">Proudly published with <a href="http://ghost.org"><span>Ghost</span></a></h5>

  </footer>

  
  <script type="text/javascript">
/* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
var disqus_shortname = '1ambda'; // required: replace example with your forum shortname

/* * * DON'T EDIT BELOW THIS LINE * * */
(function () {
 var s = document.createElement('script'); s.async = true;
 s.type = 'text/javascript';
 s.src = '//' + disqus_shortname + '.disqus.com/count.js';
 (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
 }());
</script>



  </body>
  </html>
