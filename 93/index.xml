<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>93s on Old Lisper</title>
    <link>https://1ambda.github.io/93/</link>
    <description>Recent content in 93s on Old Lisper</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.</copyright>
    <lastBuildDate>Sat, 25 Jun 2016 10:05:23 +0900</lastBuildDate>
    <atom:link href="https://1ambda.github.io/93/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Cloud Computing</title>
      <link>https://1ambda.github.io/93/cloud-computing/</link>
      <pubDate>Sat, 25 Jun 2016 10:05:23 +0900</pubDate>
      
      <guid>https://1ambda.github.io/93/cloud-computing/</guid>
      <description>

&lt;h2 id=&#34;cloud-computing&#34;&gt;Cloud Computing&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;cloud-computing-1&#34;&gt;Cloud Computing&lt;/a&gt; - MapReduce&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;cloud-computing-2&#34;&gt;Cloud Computing&lt;/a&gt; - Gossip Protocol&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;cloud-computing-3&#34;&gt;Cloud Computing&lt;/a&gt; - Membership Protocol&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;cloud-computing-4&#34;&gt;Cloud Computing&lt;/a&gt; - P2P Systems&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;cloud-computing-5&#34;&gt;Cloud Computing&lt;/a&gt; - Global Snapshot&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;cloud-computing-6&#34;&gt;Cloud Computing&lt;/a&gt; - Multicast&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;cloud-computing-7&#34;&gt;Cloud Computing&lt;/a&gt; - Paxos&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CC 01: Map Reduce</title>
      <link>https://1ambda.github.io/93/cloud-computing/cloud-computing-1/</link>
      <pubDate>Sat, 25 Jun 2016 14:42:29 +0900</pubDate>
      
      <guid>https://1ambda.github.io/93/cloud-computing/cloud-computing-1/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://ook.co/wp-content/uploads/cloudcomputing.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;intro&#34;&gt;Intro&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;map&lt;/em&gt; 과 &lt;em&gt;reduce&lt;/em&gt; 라는 단어는 &lt;em&gt;functional language&lt;/em&gt; 에서 왔다.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;map:&lt;/em&gt; processes each record sequentially and independently&lt;/li&gt;
&lt;li&gt;&lt;em&gt;reduce:&lt;/em&gt; processes set of all records in batches&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-lisp&#34;&gt;(map square &#39;(1 2 3 4))
;; (1 4 9 16)

(reduce + &#39;(1 4 9 16))
;; (+16 (+9 (+4 1)))
;; 30
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;mapreduce&#34;&gt;MapReduce&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;http://webmapreduce.sourceforge.net/docs/User_Guide/images/map-reduce.png&#34; alt=&#34;&#34; /&gt;
&lt;p align=&#34;center&#34;&gt;(&lt;a href=&#34;http://webmapreduce.sourceforge.net/&#34;&gt;http://webmapreduce.sourceforge.net/&lt;/a&gt;)&lt;/p&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Map:&lt;/em&gt; &lt;strong&gt;Parallelly&lt;/strong&gt; process &lt;strong&gt;a large number&lt;/strong&gt; of individual records to generate intermediate key/value pairs
&lt;br/&gt;
&lt;em&gt;Reduce:&lt;/em&gt; processes and merges all intermediate values associated per key&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;각 키는 하나의 &lt;em&gt;reducer&lt;/em&gt; 에 할당되고, &lt;em&gt;partitioning keys&lt;/em&gt; 에 의해 &lt;em&gt;reduce&lt;/em&gt; 가 진행된다. 자주 쓰이는 기법으로 &lt;em&gt;hash partitioning&lt;/em&gt; 이 있다. &lt;code&gt;hash(key) % # of reduce servers&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public static class MapClass extends MapReduceBase 
            implements Mapper&amp;lt;LongWriteable, Text, Text, IntWritable&amp;gt; {

  private final static IntWritable one = new IntWritable(1);
  private Text word = new Text();
  
  public void map(LongWritable key, Text value, 
                  OutputCollector&amp;lt;Text, IntWritable&amp;gt; output,
                  Reporter reporter) throws IOException {
  
    String line = value.toString();
    StringTokenizer itr = new StringTokenizer(line);
    
    while (itr.hasMoreTokens()) {
      word.set(itr.nextToken());
      output.collect(word, one);
    }
  }
}

public static class ReduceClass extends MapReduceBase
            implements Reducer&amp;lt;Text, IntWritable, Text, IntWritable&amp;gt; {
            
  public void reduce(Text key, Iterator&amp;lt;IntWritable&amp;gt; values,
                     OutputCollector&amp;lt;Text, IntWritable&amp;gt; output,
                     Reporter reporter) throw IOException {
    
    int sum = 0;
    while (values.hasNext()) {
      sum += values.next().get();
    }
    
    output.collect(key, new IntWritable(sum));
  }                     
}

public void run(String inputPath, String outputPath) throw Exception {

  // The job
  JobConf conf = new JobConf(WordCount.class);
  conf.setJobName(&amp;quot;mywordcount&amp;quot;);
  
  // The keys are words
  (srings) conf.setOutputKeyClass(Text.class);
  
  // The values are counts (ints)
  conf.setOutputValueClass(IntWritable.class);
  conf.setMapperClass(MapClass.class);
  conf.setReducerClass(ReduceClass.class);
  
  FileInputFormat.addInputPat(conf, new Path(inputPath);
  FileOutputFormat.setOutputPath(conf, new Path(outputPath));
  
  JobClient.runJob(conf);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;mapreduce-application&#34;&gt;MapReduce Application&lt;/h3&gt;

&lt;p&gt;(1) &lt;strong&gt;Distributed Grep&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;input:&lt;/em&gt; large set of files&lt;/li&gt;
&lt;li&gt;&lt;em&gt;output:&lt;/em&gt; lines that match pattern&lt;/li&gt;
&lt;li&gt;&lt;em&gt;map:&lt;/em&gt; emits a line if it matches the supplied pattern&lt;/li&gt;
&lt;li&gt;&lt;em&gt;reduce:&lt;/em&gt; copies the intermediate data to output&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(2) &lt;strong&gt;Reverse Web-Link Graph&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;input:&lt;/em&gt; web graph(tuple &lt;code&gt;(a,b)&lt;/code&gt; where page &lt;code&gt;a&lt;/code&gt; -&amp;gt; page &lt;code&gt;b&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;output:&lt;/em&gt; for each page, list of pages that link to it&lt;/li&gt;
&lt;li&gt;&lt;em&gt;map:&lt;/em&gt; process we log and for each input &lt;code&gt;&amp;lt;source, target&amp;gt;&lt;/code&gt;, it outputs &lt;code&gt;&amp;lt;target, source&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;reduce:&lt;/em&gt; emits &lt;code&gt;&amp;lt;target, list(source)&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(3) &lt;strong&gt;Count of URL Access Frequency&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;input:&lt;/em&gt; log of accessed URLs&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;em&gt;output:&lt;/em&gt; for each URL, the number of total accesses for that URL&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;em&gt;map:&lt;/em&gt; process web log and outputs &lt;code&gt;&amp;lt;URL, 1&amp;gt;&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;em&gt;multiple reducers:&lt;/em&gt; emits `&lt;URL, URL_count&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;chain another MapReduce job to calculate&lt;/strong&gt; &lt;code&gt;overall_count&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(4) &lt;em&gt;Sort&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;map&lt;/em&gt; task&amp;rsquo;s output is sorted (e.g., &lt;em&gt;quicksort&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;reduce&lt;/em&gt; task&amp;rsquo;s input is osrted (e.g., &lt;em&gt;mergesort&lt;/em&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;따라서 정렬을 하기 위해&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;map:&lt;/em&gt; &lt;code&gt;&amp;lt;key, value&amp;gt;&lt;/code&gt; -&amp;gt; &lt;code&gt;&amp;lt;value, _&amp;gt;&lt;/code&gt; (identity)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;reduce:&lt;/em&gt; &lt;code&gt;&amp;lt;key, value&amp;gt;&lt;/code&gt; -&amp;gt; &lt;code&gt;&amp;lt;key, value&amp;gt;&lt;/code&gt; (identity)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이 때 &lt;em&gt;parttition key&lt;/em&gt; 로 &lt;em&gt;range&lt;/em&gt; 를 사용하는 것이 가능하다. 다만, 특정 구간에 &lt;em&gt;data&lt;/em&gt; 가 몰려있을 수 있으므로 &lt;em&gt;dstiribution&lt;/em&gt; 을 고려해 &lt;em&gt;reducer&lt;/em&gt; 에게 할당해주면 된다.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;scheduling&#34;&gt;Scheduling&lt;/h3&gt;

&lt;p&gt;일반 &lt;em&gt;user&lt;/em&gt; 는&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Write a Map program, write a Reduce program&lt;/li&gt;
&lt;li&gt;Submit job; wait for result&lt;/li&gt;
&lt;li&gt;Need to know nothing about parallel/distributed programming&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그러나 내부적으로는&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Parallelize Map&lt;/li&gt;
&lt;li&gt;Transfer data from Map to Reduce&lt;/li&gt;
&lt;li&gt;Parallelize Reduce&lt;/li&gt;
&lt;li&gt;Implement Stroage for Map input, Map output, Reduce input, Reduce output&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그리고 &lt;em&gt;reduce&lt;/em&gt; 가 시작되기 전에 반드시 &lt;em&gt;map&lt;/em&gt; 이 끝나야 한다. 다시 말해서 &lt;em&gt;map phase&lt;/em&gt; 와 &lt;em&gt;reduce phase&lt;/em&gt; 사이에는 &lt;em&gt;barrier&lt;/em&gt; 가 있어야 한다. 그렇지 않으면 결과가 부정확할 수 있다.&lt;/p&gt;

&lt;p&gt;이제 하나하나씩 살펴보자.&lt;/p&gt;

&lt;p&gt;(1) &lt;em&gt;Parallelize Map:&lt;/em&gt; Easy. Each map task is independent of the other&lt;/p&gt;

&lt;p&gt;(2) &lt;em&gt;Transfer data from Map to Reduce:&lt;/em&gt; All map output records with same key assigned to same Reduce task. Use &lt;strong&gt;Partitionning Function&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;(3) &lt;em&gt;Parallelize Reduce:&lt;/em&gt; Easy. Each reduce task is independent of the other&lt;/p&gt;

&lt;p&gt;(4) &lt;em&gt;Implement Storage for Map input, Map output, Reduce input and Reduce output:&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Map input: from &lt;strong&gt;distributed file system&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Map output: to local disk at Map node; Use &lt;strong&gt;local file systems&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Reduce input: from (multiple) remote disks; Uses local file systems&lt;/li&gt;
&lt;li&gt;Reduce output: to &lt;strong&gt;distributed file system&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;DFS 의 예로 &lt;em&gt;Google File System&lt;/em&gt;, &lt;em&gt;HDFS&lt;/em&gt; 등이 있다.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;하둡은 스케쥴러로 *YARN, Yet Another Resouce Negotiator*를 사용한다. &lt;em&gt;YARN&lt;/em&gt; 은 각 서버를 &lt;em&gt;a collection of containers&lt;/em&gt; 로 취급한다. 여기서 &lt;em&gt;container = some CPU + some Memory&lt;/em&gt; 다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;YARN&lt;/em&gt; 은 크게 3파트로 나눌 수 있는데&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Global Resource Manager(RM):&lt;/em&gt; scheduling&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Per-server Node Manager(NM):&lt;/em&gt; Daemon and server-specific functions&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Per-application(job) Application Master(AM):&lt;/em&gt; Container negotiation with RM and NMs, Detecting task failures of that job&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week1/YARN.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;container&lt;/em&gt; 가 필요하면 &lt;em&gt;AM1&lt;/em&gt; 이 &lt;em&gt;RM&lt;/em&gt; 에게 알리고, &lt;em&gt;Node B&lt;/em&gt; 의 &lt;em&gt;NM2&lt;/em&gt; 에서 &lt;em&gt;Task&lt;/em&gt; 가 끝나면, &lt;em&gt;RM&lt;/em&gt; 이 &lt;em&gt;Node A&lt;/em&gt; 의 &lt;em&gt;AM1&lt;/em&gt; 에게 사용 가능한 컨테이너가 있다는 사실을 알려 &lt;em&gt;AM1&lt;/em&gt; 이 &lt;em&gt;NM2&lt;/em&gt; 에게 컨테이너를 사용하겠다는 요청을 보내는 식이다.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;fault-tolerance&#34;&gt;Fault-Tolerance&lt;/h3&gt;

&lt;p&gt;(1) Server Failure&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;NM&lt;/em&gt; hearbeats to &lt;em&gt;RM&lt;/em&gt;. If server fails &lt;em&gt;RM&lt;/em&gt; lets all affected &lt;em&gt;AMs&lt;/em&gt; know, and &lt;em&gt;AMs&lt;/em&gt; take action&lt;/li&gt;
&lt;li&gt;&lt;em&gt;NM&lt;/em&gt; keeps track of each task running at its server. If task fails while in-progress, mark the task as idle and restart it&lt;/li&gt;
&lt;li&gt;&lt;em&gt;AM&lt;/em&gt; heartbeats to &lt;em&gt;RM&lt;/em&gt;. On failure, &lt;em&gt;RM&lt;/em&gt; restarts &lt;em&gt;AM&lt;/em&gt;, which then syncs up with its running tasks&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(2) RM Failure&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Use old checkpoints and bring up secondary &lt;em&gt;RM&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Heartbeats also used to piggyback container requests. Avoids extra mesages&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;요약하자면, &lt;em&gt;NM&lt;/em&gt;, &lt;em&gt;AM&lt;/em&gt; 은 &lt;em&gt;RM&lt;/em&gt; 에게 &lt;em&gt;heartbeat&lt;/em&gt; 를 보낸다. &lt;em&gt;NM&lt;/em&gt; 에서 오류가 나면 &lt;em&gt;RM&lt;/em&gt; 이 영향을 받는 &lt;em&gt;AM&lt;/em&gt; 에게 알리고, 해당 &lt;em&gt;AM&lt;/em&gt; 이 적절히 처리한다. 또한 &lt;em&gt;NM&lt;/em&gt; 은 &lt;em&gt;task&lt;/em&gt; 를 유지하면서, &lt;em&gt;task&lt;/em&gt; 에러가 발생하면 재시작한다. &lt;em&gt;AM&lt;/em&gt; 에서 오류가 나면 &lt;em&gt;RM&lt;/em&gt; 이 재시작하고, 해당 &lt;em&gt;AM&lt;/em&gt; 의 태스크와 싱크를 맞춘다. &lt;em&gt;RM&lt;/em&gt; 에서 오류가 날 경우엔 &lt;em&gt;secondary RM&lt;/em&gt; 을 이용한다.&lt;/p&gt;

&lt;h3 id=&#34;stragglers&#34;&gt;Stragglers&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;slow nodes&lt;/em&gt; 를 부르는 다른말이다. &lt;em&gt;speculative execution&lt;/em&gt; 으로 해결할 수 있다. 보통 느린 이유는 &lt;em&gt;disk&lt;/em&gt;, &lt;em&gt;network bandwidth&lt;/em&gt;, &lt;em&gt;CPU&lt;/em&gt;, &lt;em&gt;memory&lt;/em&gt; 등 때문인데 &lt;em&gt;task&lt;/em&gt; 를 복제해서 다른 &lt;em&gt;node&lt;/em&gt; 에서 돌린 뒤 먼저 완료되는 노드의 결과를 이용하는 방식이다.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Perform backup (replicated) execution of straggler task: task considered done when first replica completed&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;locality&#34;&gt;Locality&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;cloud&lt;/em&gt; 의 &lt;em&gt;hierarchical topology&lt;/em&gt; 때문에 &lt;em&gt;GFS&lt;/em&gt;, &lt;em&gt;HDFS&lt;/em&gt; 등은 각 &lt;em&gt;chunk&lt;/em&gt; 를 3군데에 복제한다. 이때 같은 &lt;em&gt;rack&lt;/em&gt; 에 위치할수도 아닐수도 있다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;MapReduce&lt;/em&gt; 연산에서는 &lt;em&gt;map task&lt;/em&gt; 를 스케쥴링할때 가능하면 다음의 순서로 배치한다.&lt;/p&gt;

&lt;p&gt;(1) &lt;em&gt;chunk&lt;/em&gt; 가 있는 머신에 or failing that&lt;br /&gt;
(2) 아니면 같은 &lt;em&gt;rack&lt;/em&gt; 에 or failing that&lt;br /&gt;
(3) Anywhere&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;

&lt;p&gt;(1) MapReduce uses parallelization + aggregation to schedule applications across clusters.&lt;/p&gt;

&lt;p&gt;(2) Need to deal with failure&lt;/p&gt;

&lt;p&gt;(3) Plenty of ongoing research work in scheduling and fault-tolerance for Mapreduce and Hadoop&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;refs&#34;&gt;Refs&lt;/h3&gt;

&lt;p&gt;(1) &lt;a href=&#34;http://ook.co/solutions/cloud-computing/&#34;&gt;Title Image&lt;/a&gt;&lt;br /&gt;
(2) &lt;strong&gt;Cloud Computing Concept 1&lt;/strong&gt; by &lt;em&gt;Indranil Gupta&lt;/em&gt;, Coursera&lt;br /&gt;
(3) &lt;a href=&#34;http://webmapreduce.sourceforge.net/docs/User_Guide/sect-User_Guide-Introduction-What_is_Map_Reduce.html&#34;&gt;MapReduce Image&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CC 02: Gossip Protocol</title>
      <link>https://1ambda.github.io/93/cloud-computing/cloud-computing-2/</link>
      <pubDate>Sat, 25 Jun 2016 14:42:40 +0900</pubDate>
      
      <guid>https://1ambda.github.io/93/cloud-computing/cloud-computing-2/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://ook.co/wp-content/uploads/cloudcomputing.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;multicast&#34;&gt;Multicast&lt;/h3&gt;

&lt;p&gt;이번시간에 배울 내용은 &lt;em&gt;Gossip Protocol&lt;/em&gt; (혹은 &lt;em&gt;Epidemic Protocol&lt;/em&gt;) 입니다.&lt;/p&gt;

&lt;p&gt;기존에는 특정 그룹에게 메세지를 보내기 위해 &lt;em&gt;multicast&lt;/em&gt; 를 이용했지만, 클라우드 컴퓨티 환경에서는&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;프로세스가 죽어 노드가 크래쉬를 일으킬수도&lt;/li&gt;
&lt;li&gt;네트워크 문제때문에 패킷이 딜레이되거나, 드랍될 수 있고&lt;/li&gt;
&lt;li&gt;노드가 빠르게 증가합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그러나 멀티캐스트는 &lt;em&gt;fault-tolerance&lt;/em&gt; 와 &lt;em&gt;scalability&lt;/em&gt; 측면에서 부족한 부분이 많았습니다. 이런 문제를 해결하기 위해 다양한 방법이 도입되었는데&lt;/p&gt;

&lt;p&gt;(1) &lt;strong&gt;Centralized:&lt;/strong&gt; 중앙 서버에서 &lt;em&gt;TCP, UDP&lt;/em&gt; 패킷을 날립니다. 간단한 구현이지만 중앙서버의 오버헤드가 높고, 수천개의 노드가 있을때 &lt;em&gt;latency&lt;/em&gt; 가 생깁니다. 노드의 수를 &lt;code&gt;N&lt;/code&gt; 이라 했을때, 모든 노드에 메시지가 전달되는데 &lt;code&gt;O(N)&lt;/code&gt; 시간이 걸리지요.&lt;/p&gt;

&lt;p&gt;(2) &lt;strong&gt;Tree-Based:&lt;/strong&gt; 전달 받은 노드에서, 다시 패킷을 전달하여 경로가 &lt;em&gt;tree&lt;/em&gt; 형태로 구성됩니다. &lt;em&gt;balanced tree&lt;/em&gt; 라면 어떤 그룹에 패킷이 전달되는데 &lt;code&gt;O(logN)&lt;/code&gt; 의 시간이 걸립니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week2/tree_based_multicast.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;이 방법의 단점은&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;tree&lt;/em&gt; 를 구성하고 유지하는데 필요한 오버헤드&lt;/li&gt;
&lt;li&gt;&lt;em&gt;root&lt;/em&gt; 에 가까운 곳에서 &lt;em&gt;failure&lt;/em&gt; 가 발생했을때의 파급력&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;일반적으로 &lt;em&gt;tree-based multicast&lt;/em&gt; 프로토콜에서는 &lt;em&gt;spanning tree&lt;/em&gt; 를 구성해서 최단비용으로 패킷을 전달합니다. 그리고 메시지가 올바르게 전달되었는지 &lt;em&gt;ACK&lt;/em&gt; 또는 &lt;em&gt;NAK&lt;/em&gt; 를 이용하는데 &lt;em&gt;SRM&lt;/em&gt; 이던 &lt;em&gt;RMTP&lt;/em&gt; 던 여전히 &lt;code&gt;O(N)&lt;/code&gt; 만큼의 &lt;em&gt;ACK/NAK&lt;/em&gt; 오버헤드가 발생합니다.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;gossip&#34;&gt;Gossip&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week2/gossip_example1.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week2/gossip_example2.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;가십 프로토콜은 위 그림처럼 작동합니다.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;주기적으로 랜덤한 타겟을 골라 &lt;em&gt;gossip message&lt;/em&gt; 를 전송합니다&lt;/li&gt;
&lt;li&gt;그리고 이것을 받아 &lt;em&gt;infected&lt;/em&gt; 상태가 된 노드도 똑같이 행동합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이걸 &lt;em&gt;Push gossip&lt;/em&gt; 이라 부릅니다. &lt;em&gt;multiple message&lt;/em&gt; 를 가십하기 위해 랜덤 서브셋을 선택하거나, &lt;em&gt;recently-received&lt;/em&gt; 메시지를 를 선택하거나, &lt;em&gt;higher priority one&lt;/em&gt; 을 고를 수 있습니다.&lt;/p&gt;

&lt;p&gt;어떤 가십 메시지에 대해 대부분의 노드가 &lt;em&gt;infected&lt;/em&gt; 되었을때 &lt;em&gt;push gossip&lt;/em&gt; 은 비효율적입니다. 이때는 &lt;em&gt;uninfected&lt;/em&gt; 노드가, 새로운 가십메시지가 있는지 주변 노드에게 물어보는 &lt;strong&gt;pull gossip&lt;/strong&gt; 이 오버헤드가 더 적습니다.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Pull gossip:&lt;/strong&gt; Periodically poll a few random selected processes for new multicast meesages that you haven&amp;rsquo;t received&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;gossip-analysis&#34;&gt;Gossip Analysis&lt;/h3&gt;

&lt;p&gt;가십프로토콜은 다음의 특징을 가집니다.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;lightweight&lt;/strong&gt; in large groups&lt;/li&gt;
&lt;li&gt;spreads a multicast quickly&lt;/li&gt;
&lt;li&gt;highly &lt;em&gt;fault-tolerant&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이를 위해 간단한 증명을 해보도록 하겠습니다.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;전체 &lt;code&gt;n+1&lt;/code&gt; 의 &lt;em&gt;population&lt;/em&gt; 에 대해&lt;/li&gt;
&lt;li&gt;&lt;em&gt;uninfected individuals&lt;/em&gt; 의 수를 &lt;code&gt;x&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;infected individuals&lt;/em&gt; 의 수를 &lt;code&gt;y&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;individual pair&lt;/em&gt; 간의 &lt;em&gt;contract rate&lt;/em&gt; 를 &lt;code&gt;β&lt;/code&gt; 라 하면&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;항상 &lt;code&gt;x + y = n + 1&lt;/code&gt; 이고, 시작상태에서는 &lt;code&gt;x_0 = n, y_0 = 1&lt;/code&gt; 입니다. 그리고 시간이 지날때마다 &lt;em&gt;uninfected&lt;/em&gt; &lt;code&gt;y&lt;/code&gt; 는 다음처럼 감소합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://latex.codecogs.com/gif.latex?%5Cfrac%7B%5Cmathrm%7Bd%7D%20x%7D%7B%5Cmathrm%7Bd%7D%20t%7D%20%3D%20-%5Cbeta%20xy&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;그러면 이 수식으로부터 다음을 이끌어 낼 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://latex.codecogs.com/gif.latex?x%20%3D%20%7B%20n%28n&amp;amp;plus;1%29%20%5Cover%20%7Bn%20&amp;amp;plus;%20e%5E%7B%5Cbeta%28n&amp;amp;plus;1%29t%7D%7D%7D&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://latex.codecogs.com/gif.latex?y%20%3D%20%7B%20%28n&amp;amp;plus;1%29%20%5Cover%20%7B1%20&amp;amp;plus;%20ne%5E%7B-%5Cbeta%28n&amp;amp;plus;1%29t%7D%7D%7D&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;그리고 &lt;em&gt;infected node&lt;/em&gt; 가 랜덤하게 &lt;code&gt;b&lt;/code&gt; 개의 노드를 고른다 하면 &lt;code&gt;β&lt;/code&gt; 는&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://latex.codecogs.com/gif.latex?%5Cbeta%20%3D%20%7Bb%20%5Cover%20b%7D&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;그리고 시간 &lt;code&gt;t&lt;/code&gt; 를 가십이 진행되는 &lt;em&gt;round&lt;/em&gt; 라 보고 &lt;code&gt;t = clog(n)&lt;/code&gt; 이라 치환하겠습니다. 다음을 이끌어낼 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://latex.codecogs.com/gif.latex?y%20%5Capprox%20%28n&amp;amp;plus;1%29%20-%20%7B1%20%5Cover%20n%5E%7Bcb-2%7D%7D&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;이 식으로부터 &lt;em&gt;gossip protocol&lt;/em&gt; 이 &lt;em&gt;low latency&lt;/em&gt;, &lt;em&gt;reliability&lt;/em&gt;, &lt;em&gt;lightweight&lt;/em&gt; 하다는 것을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;(1) &lt;strong&gt;low latency&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;c, b&lt;/code&gt; 를 &lt;code&gt;n&lt;/code&gt; 과 독립적으로 아주 작은 숫자로 세팅하면 &lt;code&gt;clog(n)&lt;/code&gt; &lt;em&gt;round&lt;/em&gt; 이므로 적은 시간 내에 메시지가 전파됩니다.&lt;/p&gt;

&lt;p&gt;(2) &lt;strong&gt;reliability&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;n&lt;/code&gt; 이 매우 크면 &lt;code&gt;1 / n^{cb-2}&lt;/code&gt; 가 &lt;code&gt;0&lt;/code&gt; 에 가까워지므로, &lt;code&gt;1 / n^{cb-2}&lt;/code&gt; 만큼의 노드를 제외한 모든 노드가 &lt;em&gt;infected&lt;/em&gt; 된다는 것을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;(3) &lt;strong&gt;lightweight&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;각 노드는 &lt;code&gt;cb log(n)&lt;/code&gt; 만큼의 &lt;em&gt;gossip message&lt;/em&gt; 만 전파합니다. 이론적으로는 &lt;code&gt;log(N)&lt;/code&gt; 은 상수가 아니지만, 실제로는 아주 천천히 증가하는 숫자기에 작은 숫자처럼 생각할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;fault-tolerance&#34;&gt;Fault-Tolerance&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;50% packet loss&lt;/em&gt; 를 생각해 봅시다. &lt;code&gt;b&lt;/code&gt; 를 &lt;code&gt;2/b&lt;/code&gt; 로 치환하면 됩니다. 그러면 이전과 같은 &lt;em&gt;reliability&lt;/em&gt; &lt;em&gt;0% packet loss&lt;/em&gt; 를 위하 두배의 &lt;em&gt;round&lt;/em&gt; 만큼만 더 진행하면 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;node failure&lt;/em&gt; 는 어떨까요? 50% 노드에서 &lt;em&gt;failure&lt;/em&gt; 가 발생한다면 &lt;code&gt;n, b&lt;/code&gt; 을 &lt;code&gt;2/n, 2/b&lt;/code&gt; 으로 치환하면 됩니다. 이는 &lt;em&gt;contract rate&lt;/em&gt; 에서 가십 메시지를 전달하는 &lt;code&gt;n&lt;/code&gt; 중 &lt;code&gt;2/n&lt;/code&gt; 의 노드만 살아있고, 선택되는 &lt;code&gt;b&lt;/code&gt; 중 &lt;code&gt;b/2&lt;/code&gt; 노드만 살아있기 때문입니다. 이 경우에도 상수만 곱하면 이전과 같은 &lt;em&gt;reliability&lt;/em&gt; 를 얻을 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;failure&lt;/em&gt; 와 관련해서 한 가지 생각해 볼 문제가 있습니다. 모든 노드가 죽는것이 가능할까요? 물론 가능합니다 초기에 모든 노드가 죽으면요. 그러나 &lt;em&gt;improbable&lt;/em&gt; 입니다. 일단 몇개의 노드가 &lt;em&gt;infected&lt;/em&gt; 되면, 이후에는 퍼지는 속도가 훨씬 더 빠르기 때문입니다. 루머나 바이러스가 퍼질 수 있는 이유를 생각하면 이해하기 쉽습니다.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;pull-gossip&#34;&gt;Pull Gossip&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week2/pull_gossip_analysis.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;그림에서 볼 수 있듯이, 어떤 형태의 가십 프로토콜이던 &lt;code&gt;2/N&lt;/code&gt; 까지 전달할때는 &lt;code&gt;O(logN)&lt;/code&gt; 만큼의 시간이 걸립니다. 그 이후에는 &lt;em&gt;pull gossip&lt;/em&gt; 이 훨씬 빠르죠.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;i&lt;/code&gt; &lt;em&gt;round&lt;/em&gt; 후에 남아있는 &lt;em&gt;uninfected node&lt;/em&gt; 의 수를 &lt;code&gt;p_i&lt;/code&gt; 라 합시다. &lt;em&gt;pull gossip&lt;/em&gt; 을 이용할때 다음 단계에서도 &lt;em&gt;uninfected&lt;/em&gt; 일 확률은&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://latex.codecogs.com/gif.latex?p_%7Bi&amp;amp;plus;1%7D%20%3D%20p_i%5E%7Bk&amp;amp;plus;1%7D&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;이는 &lt;code&gt;p_i&lt;/code&gt; 자체가 &lt;em&gt;uninfected&lt;/em&gt; 여야 하고, 이 노드가 선택하는 &lt;code&gt;k = b&lt;/code&gt; 만큼의 노드도 &lt;em&gt;uninfected&lt;/em&gt; 여야 하는데, 이 확률은 극히 낮습니다. 슬라이드에서 보듯이 &lt;em&gt;super-exponential&lt;/em&gt; 하고, 그렇기 때문에 &lt;em&gt;second half&lt;/em&gt; 부터는 &lt;em&gt;pull gossip&lt;/em&gt; 이 &lt;code&gt;O(log(logN))&lt;/code&gt; 입니다.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;topology-aware-gossip&#34;&gt;Topology-Aware Gossip&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week2/topology_aware_gossip.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;만약 &lt;em&gt;uninfected node&lt;/em&gt; 를 &lt;em&gt;uniformly random&lt;/em&gt; 하게 고른다면 위 그림에서 라우터의 오버헤드는 &lt;code&gt;O(N)&lt;/code&gt; 이 됩니다. 더 정확하게는 &lt;em&gt;round&lt;/em&gt; 마다 &lt;code&gt;b * (2/n)&lt;/code&gt; 이 될겁니다.&lt;/p&gt;

&lt;p&gt;이를 해결하기 위해, 서브넷에 &lt;code&gt;n_i&lt;/code&gt; 개의 노드가 있을때 자신이 속한 서브넷에 있는 &lt;em&gt;uninfected node&lt;/em&gt; 를 더 자주 고르게, 확률을 &lt;code&gt;1 - (1/n_i)&lt;/code&gt; 가 되도록 합니다. 그러면, 현재 서브넷에 있는 노드를 선택할 확률이 1 에 가까우므로 &lt;code&gt;O(logN)&lt;/code&gt; 시간 내에 전파되고, 라우터의 오버헤드는 &lt;code&gt;(n_i) / (n_i)&lt;/code&gt; 가 되어, &lt;code&gt;O(1)&lt;/code&gt; 이 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;refs&#34;&gt;Refs&lt;/h3&gt;

&lt;p&gt;(1) &lt;a href=&#34;http://ook.co/solutions/cloud-computing/&#34;&gt;Title Image&lt;/a&gt;&lt;br /&gt;
(2) &lt;strong&gt;Cloud Computing Concept 1&lt;/strong&gt; by &lt;em&gt;Indranil Gupta&lt;/em&gt;, Coursera&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CC 03: Membership Protocol</title>
      <link>https://1ambda.github.io/93/cloud-computing/cloud-computing-3/</link>
      <pubDate>Sat, 25 Jun 2016 14:42:41 +0900</pubDate>
      
      <guid>https://1ambda.github.io/93/cloud-computing/cloud-computing-3/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://ook.co/wp-content/uploads/cloudcomputing.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;왜 &lt;em&gt;membership&lt;/em&gt; 이란 개념이 클라우드 컴퓨팅에 필요할까요?&lt;/p&gt;

&lt;p&gt;한 노드가 &lt;em&gt;OS&lt;/em&gt;, &lt;em&gt;Disk&lt;/em&gt;, &lt;em&gt;Network&lt;/em&gt; 등 때문에 10년 (120개월) 마다 한 번씩 고장난다고 합시다. 그러면 120개의 노드를 가지고 있다면 1개월마다  한 번씩입니다. 이정도는 참을만하죠? 그런데, 12,000 개의 서버를 가지고 있다면 &lt;em&gt;MTTF (mean time to failure)&lt;/em&gt; 는 7.2 시간마다 한번씩입니다. 이건 큰 문제입니다.&lt;/p&gt;

&lt;p&gt;따라서 머신이 멀쩡한지 아닌지를 수동이 아니라 자동으로 판단하고 보고해줄 시스템이 필요합니다. &lt;em&gt;membership&lt;/em&gt; 이 필요한 것이죠. 이 대상은&lt;/p&gt;

&lt;p&gt;(1) Process &lt;em&gt;group&lt;/em&gt;-based systems&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Clouds / Datacenters&lt;/li&gt;
&lt;li&gt;Replicated servers&lt;/li&gt;
&lt;li&gt;Distributed databases&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(2) Cash-stop / Fail stop process failures&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week2/membership1.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week2/membership2.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;멤버십 프로토콜은 다음처럼 구성되어 있습니다.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;멤버쉽 리스트 (&lt;em&gt;complete&lt;/em&gt;, &lt;em&gt;almost-complete&lt;/em&gt;, &lt;em&gt;partial-random&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;dissemination&lt;/em&gt; mechanism to inform about joins, leavs, and failures of processes&lt;/li&gt;
&lt;li&gt;&lt;em&gt;failure detector&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;failure-detector&#34;&gt;Failure Detector&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;distributed failure detector&lt;/em&gt; 를 평가할 수 있는 지표는&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Completeness:&lt;/strong&gt; each failure is detected&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accuracy:&lt;/strong&gt; there is no mistaken detection&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Speed:&lt;/strong&gt; time to first detection of a failure&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Scale:&lt;/strong&gt; equal load on each member. network message load&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;안타깝게도 &lt;em&gt;completeness&lt;/em&gt; 와 &lt;em&gt;accuracy&lt;/em&gt; 를 &lt;em&gt;lossy network&lt;/em&gt; 에서 동시에 추구할 수 없다는 사실이 밝혀졌습니다. (&lt;em&gt;Chandra and Toueg&lt;/em&gt;)&lt;/p&gt;

&lt;p&gt;현실적으로는&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;completeness:&lt;/em&gt; 100% guaranteed&lt;/li&gt;
&lt;li&gt;&lt;em&gt;accuracy:&lt;/em&gt; partial / probabilistic guarantee&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;(1) Centralized Heartbeating&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week2/centralized_heartbeating.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;중앙 집중형이기 때문에 &lt;em&gt;load&lt;/em&gt; 가 한쪽으로만 쏠린다는 단점이 있습니다.&lt;/p&gt;

&lt;p&gt;(2) Ring Heartbeating&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week2/ring_heartbeating.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;링 형태로 구성되었기때문에 동시에 발생하는 다수개의 &lt;em&gt;failure&lt;/em&gt; 를 탐지하지 못합니다.&lt;/p&gt;

&lt;p&gt;(3) All To All Heartbeating&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week2/all2all_heartbeating.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;우선 &lt;em&gt;equal load&lt;/em&gt; 라는 장점이 있습니다. 개별 노드당 오버헤드가 큰 것처럼 보이는데, 뒤에서 다시 한번 보겠지만 사실 그렇게 크지 않습니다.&lt;/p&gt;

&lt;p&gt;(4) Gossip-Style Membership&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week2/gossip_heartbeating1.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week2/gossip_heartbeating2.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;accuracy&lt;/em&gt; 가 높다는 장점이 있습니다.&lt;/p&gt;

&lt;p&gt;동작 방식은 이렇습니다. &lt;em&gt;hearbeat&lt;/em&gt; 가 &lt;code&gt;T_fail&lt;/code&gt; 초 후에도 증가하지 않으면, 해당 멤버는 &lt;em&gt;failure&lt;/em&gt; 를 일으킨 것으로 판별됩니다. 그리고 멤버 리스트에서는 &lt;code&gt;T_cleanup&lt;/code&gt; 초 후에 제거됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week2/why_cleanup_time1.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week2/why_cleanup_time2.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week2/why_cleanup_time3.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;왜 바로 제거하지 않고, &lt;code&gt;T_cleanup&lt;/code&gt; 초 후에 제거할까요? 이는 위 슬라이드에서 볼 수 있듯이 &lt;code&gt;3&lt;/code&gt; 번 노드가 &lt;em&gt;failure&lt;/em&gt; 를 일으켰을때, &lt;code&gt;2&lt;/code&gt; 번 노드의 멤버 리스트에서 바로 제거한다면 &lt;code&gt;1&lt;/code&gt; 번 노드로부터 업데이트를 받아 멤버 리스트에 &lt;em&gt;failure&lt;/em&gt; 가 발생하지 않은것처럼 추가될 수 있기 때문입니다.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week2/gossip_membership_analysis.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;T_gossip&lt;/code&gt; 이 줄면, &lt;em&gt;bandwidth&lt;/em&gt; 를 많이 잡아먹는 대신, &lt;em&gt;detection time&lt;/em&gt; 이 줄어듭니다. &lt;em&gt;trade-off&lt;/em&gt; 라 보면 되겠습니다.&lt;/p&gt;

&lt;p&gt;그리고 &lt;code&gt;T_fail, T_cleanup&lt;/code&gt; 이 증가하면 &lt;em&gt;false positive rate&lt;/em&gt; 는 줄어드는 대신, 당연히 &lt;em&gt;detection time&lt;/em&gt; 이 늘어납니다.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;그러면 위에 나온 것 중 어느것이 가장 좋은 &lt;em&gt;failure detector&lt;/em&gt; 일까요? 앞서 언급했던 기준들을 이용해서 살펴보겠습니다.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Completeness:&lt;/em&gt; guarantee always&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Accuracy:&lt;/em&gt; a prob of mstake in time T &lt;code&gt;PM(T)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Speed:&lt;/em&gt; &lt;code&gt;T&lt;/code&gt; time units&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Scale:&lt;/em&gt; &lt;code&gt;N*L&lt;/code&gt; Compare this across protocols&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(1) All-To-All Heartbeating&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week2/performance_all2all.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;work load&lt;/em&gt; 가 &lt;code&gt;N&lt;/code&gt; 에 비례합니다.&lt;/p&gt;

&lt;p&gt;(2) Gossip-Style Heartbeating&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week2/performance_gossip.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;tg&lt;/code&gt; 를 &lt;code&gt;O(n)&lt;/code&gt; 의 &lt;em&gt;gossip message&lt;/em&gt; 를 보내는데 걸리는 &lt;em&gt;gossip period&lt;/em&gt; 라 했을때, 한 &lt;em&gt;round&lt;/em&gt; 에서의 전파 시간인 &lt;code&gt;logN&lt;/code&gt; 을 곱해 &lt;code&gt;T = logN * tg&lt;/code&gt; 입니다. 이때 오버헤드 &lt;code&gt;L = N/tg&lt;/code&gt; 이므로, &lt;code&gt;L = N * logN / T&lt;/code&gt; 입니다.&lt;/p&gt;

&lt;p&gt;오버헤드가 &lt;em&gt;all-to-all heartbeating&lt;/em&gt; 보다 훨씬 높죠? 이는 &lt;em&gt;accuracy&lt;/em&gt; 가 더 높기 때문입니다. 앞에서 &lt;em&gt;all-to-all&lt;/em&gt; 가 더 비용이 많이 들것 같지만 실제로는 그렇지 않다고 했었는데, 이런 이유에서입니다.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week2/suboptimal_worstcase.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;worst case load per member&lt;/em&gt; &lt;code&gt;L*&lt;/code&gt; 라 하고&lt;/li&gt;
&lt;li&gt;&lt;code&gt;P_ml&lt;/code&gt; 을 독립적인 메시지 손실양 이라고 했을때,&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;L*&lt;/code&gt; 을 &lt;code&gt;T, PM(T), P_ml&lt;/code&gt; 의 함수로 표시하면&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://latex.numberempire.com/render?L%2A%20%3D%20%7B%20log%28PM%28T%29%29%20%5Cover%20log%28P_ml%29%20%7D%20%2A%20%7B1%20%5Cover%20T%20%7D&amp;amp;sig=b21744720873bd544c3b394bd827b158&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;메시지 손실 &lt;code&gt;P_ml&lt;/code&gt; 이 높을수록, 오버헤드 &lt;code&gt;L*&lt;/code&gt; 는 당연히 작아져야 하고, &lt;code&gt;PM(T)&lt;/code&gt; 가 높을수록 &lt;em&gt;false-positive&lt;/em&gt; 가 많으므로 오버헤드가 높습니다. 수식을 보면 변수 &lt;code&gt;N&lt;/code&gt; 이 없는데, 이는 &lt;em&gt;scale-free&lt;/em&gt; 함을 보여줍니다.&lt;/p&gt;

&lt;p&gt;그리고 &lt;em&gt;all-to-all&lt;/em&gt; 이나 &lt;em&gt;gossip-based&lt;/em&gt; 는 &lt;strong&gt;suboptimal&lt;/strong&gt; 입니다. 왜냐하면&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;L = O(N/T)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;try to achieve simultaneous detection at all processes&lt;/li&gt;
&lt;li&gt;fail to distinguish &lt;strong&gt;failure detection&lt;/strong&gt; and &lt;strong&gt;dissemination components&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;따라서 두개의 컴포넌트를 분리하고, &lt;em&gt;non heatbeat-based failure detection&lt;/em&gt; 을 이용하면 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;swim-failure-detector&#34;&gt;SWIM Failure Detector&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week2/SWIM_intro.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;SWIM&lt;/em&gt; 은 &lt;em&gt;probabilistic failure detector protocol&lt;/em&gt; 입니다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;period&lt;/em&gt; &lt;code&gt;T&lt;/code&gt; 동안 프로세스(노드) &lt;code&gt;pi&lt;/code&gt; 는 &lt;code&gt;pj&lt;/code&gt; 를 랜덤하게 골라 &lt;em&gt;ping&lt;/em&gt; 을 보냅니다. &lt;em&gt;ack&lt;/em&gt; 가 오면, 남은 &lt;em&gt;period&lt;/em&gt; 동안 아무것도 하지 않습니다. 그러나 위 슬라이드에서 볼 수 있듯이 &lt;code&gt;pj&lt;/code&gt; 가 응답하지 않으면 랜덤하게 &lt;code&gt;K&lt;/code&gt; 개의 프로세스를 선택해서, &lt;em&gt;ping&lt;/em&gt; 을 날리고, 이를 통해 &lt;em&gt;indirect&lt;/em&gt; 한 방법으로 &lt;code&gt;pj&lt;/code&gt; 의 응답을 검사합니다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;SWIM&lt;/em&gt; 의 퍼포먼스는 &lt;em&gt;heartbeat&lt;/em&gt; 와 비교했을때 어떨까요?&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week2/SWIM_vs_heartbeating.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;X&lt;/code&gt; 축은 &lt;strong&gt;process load&lt;/strong&gt;, &lt;code&gt;Y&lt;/code&gt; 축은 &lt;em&gt;first detection time&lt;/em&gt; 입니다. &lt;em&gt;false-positive rate&lt;/em&gt; 와 &lt;em&gt;message loss rate&lt;/em&gt; 는 고정되어있다고 가정합니다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;heartbeat&lt;/em&gt; 의 경우에는 앞서 봤듯이 &lt;em&gt;detection time&lt;/em&gt; 읖 높이면 &lt;em&gt;work load&lt;/em&gt; 가 낮아지고 (= &lt;em&gt;low bound on the bandwidth&lt;/em&gt;), 반대로 &lt;em&gt;detection time&lt;/em&gt; 을 낮추면, &lt;em&gt;work load&lt;/em&gt; 가 높아집니다. 반면 &lt;em&gt;SWIM&lt;/em&gt; 은 둘다 적죠.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week2/SWIM_parameters.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;슬라이드에서 볼 수있듯이 &lt;em&gt;first detection time&lt;/em&gt;, &lt;em&gt;process load&lt;/em&gt; 는 &lt;em&gt;constant&lt;/em&gt; 입니다. &lt;em&gt;process load&lt;/em&gt; 의 경우에는 &lt;em&gt;15% packet loss&lt;/em&gt; 가 있을때 &lt;em&gt;optimal&lt;/em&gt; 의 8배인 &lt;code&gt;8L*&lt;/code&gt; 보다 적습니다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;false positive rate&lt;/em&gt; 는 &lt;code&gt;K&lt;/code&gt; 를 증가시켜서 줄일 수 있습니다. &lt;code&gt;K&lt;/code&gt; 가 증가함에 따라 &lt;em&gt;false positive rate&lt;/em&gt; 는 지수적으로 감소합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week2/SWIM_accuracy_load.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;del&gt;쿨하게 페이퍼를 보시라는 교수님&lt;/del&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week2/SWIM_detection_time.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;어째서 &lt;em&gt;expected detection time&lt;/em&gt; 이 &lt;code&gt;1 / e-1&lt;/code&gt; 일까요? 하나의 프로세스가 죽었을때, 핑 되려면 다른 프로세스의 멤버쉽 리스트에 있어야 하고, 랜덤하게 선택되야 합니다.&lt;/p&gt;

&lt;p&gt;랜덤하게 선택될 확률은 &lt;code&gt;1/N&lt;/code&gt; 이고, 선택되지 않을 확률은 &lt;code&gt;1 - 1/N&lt;/code&gt; 입니다. 다른 &lt;code&gt;N-1&lt;/code&gt; 개의 프로세스에 의해 모두 선택되지 않을 확률은 &lt;code&gt;(1-1/N)^N-1&lt;/code&gt; 이고, &lt;code&gt;1&lt;/code&gt; 에서 이 값을 빼면 선택될 확률입니다. 그리고 익히 알려진 바대로 &lt;del&gt;응?&lt;/del&gt; &lt;code&gt;N&lt;/code&gt; 이 매우 커지면 이 값은 &lt;code&gt;1-e^-1&lt;/code&gt; 과 같습니다.&lt;/p&gt;

&lt;p&gt;그리고 확률론을 잘 안다면 &lt;del&gt;응?&lt;/del&gt; 이 값에 기대값을 취하면 &lt;code&gt;e / e-1&lt;/code&gt; 이 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week2/time_bounded_completeness.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;여기에 간단한 트릭을 이용하면 &lt;em&gt;worst case&lt;/em&gt; 로 &lt;code&gt;O(N)&lt;/code&gt;, 정확히는 &lt;code&gt;2N-1&lt;/code&gt; &lt;em&gt;period&lt;/em&gt; 내에 &lt;em&gt;failure&lt;/em&gt; 가 발견되도록 할 수 있습니다. &lt;em&gt;membership list&lt;/em&gt; 를 순회하다가, 마지막에 도달하면 랜덤하게 재배열 하는 것입니다.&lt;/p&gt;

&lt;p&gt;그러면 최악의 경우 2번째 멤버에 대해 &lt;em&gt;ping&lt;/em&gt; 을 날릴때 첫번째 멤버에 &lt;em&gt;failure&lt;/em&gt; 가 발생하고, 재 배열했을때 첫번째 멤버가 마지막에 있다면 &lt;code&gt;(N-1) + (N)&lt;/code&gt; 의 &lt;em&gt;period&lt;/em&gt; 가 걸립니다. 그리고 이것은 &lt;em&gt;accuracy&lt;/em&gt; 등 다른 &lt;em&gt;failure detector&lt;/em&gt; 의 속성들을 그대로 유지한채 &lt;em&gt;worst case&lt;/em&gt; 시간을 줄이는 결과를 만듭니다.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h4 id=&#34;dissemination-and-suspicion&#34;&gt;Dissemination and Suspicion&lt;/h4&gt;

&lt;p&gt;&lt;em&gt;dissemiantion&lt;/em&gt; 방법으로&lt;/p&gt;

&lt;p&gt;(1) &lt;strong&gt;Multicast&lt;/strong&gt; (Hardware / IP)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;unreliable&lt;/li&gt;
&lt;li&gt;multiple simultaneous multicasts&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(2) &lt;strong&gt;Point-To-Point&lt;/strong&gt; (TCP / UDP)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;expensive&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(3) &lt;strong&gt;Zero extra message:&lt;/strong&gt; Piggyback on Failure Detector messages&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Infection-style Dissemination (like &lt;em&gt;SWIM&lt;/em&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week2/infection_style_dissemination.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;슬라이드에서 볼 수 있듯이 &lt;em&gt;infection style dissemination&lt;/em&gt; 은 &lt;code&gt;λ log(N)&lt;/code&gt; &lt;em&gt;protocol periods&lt;/em&gt; 후에 &lt;code&gt;N^-(2λ-2)&lt;/code&gt; 개의 프로세스만 업데이트되지 않습니다. 바꿔말하면 &lt;code&gt;O(logN)&lt;/code&gt; 후에 대부분의 프로세스는 발견돈 &lt;em&gt;failure&lt;/em&gt; 정보를 업데이트 한다는 뜻입니다.&lt;/p&gt;

&lt;p&gt;여기서 &lt;code&gt;λ&lt;/code&gt; 는 &lt;em&gt;consistency level&lt;/em&gt; 을 결정하는 상수입니다. 어떤 경우에도 &lt;em&gt;SWIM detector&lt;/em&gt; 는 &lt;em&gt;failure&lt;/em&gt; 를 &lt;code&gt;2N-1&lt;/code&gt; 내에 발견하기 때문에 &lt;em&gt;completeness&lt;/em&gt; 100% 가 보장됩니다.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;suspicion-mechanism&#34;&gt;Suspicion Mechanism&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;false positive&lt;/em&gt; 가 발생하는 이유는&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;perturbed processes&lt;/li&gt;
&lt;li&gt;package losses (e.g from congestion)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;SWIM&lt;/em&gt; 에서 사용했던 &lt;em&gt;indirect pinging&lt;/em&gt; 도 이 문제를 해결하지 못할 수 있습니다. (e.g &lt;em&gt;correlated message losses near pinged host&lt;/em&gt;)&lt;/p&gt;

&lt;p&gt;먼저 &lt;em&gt;failure&lt;/em&gt; 가 발견되었을때 다른 노드들에게 알리기 전에 먼저 &lt;em&gt;suspect&lt;/em&gt; 한다면 &lt;em&gt;false positive&lt;/em&gt; 비율을 줄일 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week2/suspicon_mechanism_state_machine.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;그림이 좀 복잡한데, 프로세스(노드) &lt;code&gt;pi&lt;/code&gt; 기준으로 &lt;em&gt;state&lt;/em&gt; 가 어떻게 변하는지를 나타낸 그림이라고 보면 됩니다. &lt;code&gt;pj&lt;/code&gt; 에게 핑을 날려 응답하지 않으면 &lt;em&gt;suspected&lt;/em&gt; 상태로 변하고, 여기서 &lt;em&gt;timeout&lt;/em&gt; 되면 &lt;em&gt;failed&lt;/em&gt; 되어 &lt;code&gt;pj&lt;/code&gt; 가 &lt;em&gt;failure&lt;/em&gt; 라고 &lt;em&gt;dissemination&lt;/em&gt; 하는 상태가 됩니다.&lt;/p&gt;

&lt;p&gt;한 가지 발생할 수 있는 문제점은 &lt;em&gt;suspected&lt;/em&gt; 상태에서 &lt;em&gt;alive&lt;/em&gt; 상태로 반복적으로 전환될 수 있다는 점입니다. 이러한 혼란을 피하기 위해 &lt;em&gt;incarnation number&lt;/em&gt; 를 사용할 수 있습니다.&lt;/p&gt;

&lt;p&gt;프로세스 &lt;code&gt;pj&lt;/code&gt; 가 &lt;em&gt;suspected&lt;/em&gt; 메세지를 받았을때, &lt;em&gt;incarnation number&lt;/em&gt; 를 증가시킬 수 있는 것은 &lt;code&gt;pj&lt;/code&gt; 만 가능합니다. 그리고 &lt;em&gt;increase incarnation number&lt;/em&gt; 메시지를 받은 다른 프로세스들은 &lt;em&gt;alive&lt;/em&gt; &lt;code&gt;pj&lt;/code&gt; 메시지를 전달합니다.&lt;/p&gt;

&lt;p&gt;높은 숫자의 &lt;em&gt;incarnation number&lt;/em&gt; 가 더 우선합니다. 그리고 &lt;em&gt;suspect&lt;/em&gt; 와 &lt;em&gt;alive&lt;/em&gt; 같은 값이라면 &lt;em&gt;suspect&lt;/em&gt; 메시지로 처리됩니다. 그리고 &lt;em&gt;failed&lt;/em&gt; 메시지는 다른 어떤 메시지보다 더 높은 우선순위를 가지고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;failures the norm, not the exception in datacenters&lt;/li&gt;
&lt;li&gt;every distributed system uses a failure detector&lt;/li&gt;
&lt;li&gt;many distributed systems use a membership service&lt;/li&gt;
&lt;li&gt;ring failure detection underlies &lt;em&gt;IBM SP2&lt;/em&gt; and many other similar clusters&lt;/li&gt;
&lt;li&gt;Gossip-style failure detection underlies AWS EC2/S3 (rumored)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;refs&#34;&gt;Refs&lt;/h3&gt;

&lt;p&gt;(1) &lt;a href=&#34;http://ook.co/solutions/cloud-computing/&#34;&gt;Title Image&lt;/a&gt;&lt;br /&gt;
(2) &lt;strong&gt;Cloud Computing Concept 1&lt;/strong&gt; by &lt;em&gt;Indranil Gupta&lt;/em&gt;, Coursera&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CC 04: P2P Systems</title>
      <link>https://1ambda.github.io/93/cloud-computing/cloud-computing-4/</link>
      <pubDate>Sat, 25 Jun 2016 14:42:44 +0900</pubDate>
      
      <guid>https://1ambda.github.io/93/cloud-computing/cloud-computing-4/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://ook.co/wp-content/uploads/cloudcomputing.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;P2P 시스템의 기술들은 &lt;em&gt;cloud computing&lt;/em&gt; 의 많은 분야에서 활용됩니다. 뒤에서 배울 &lt;em&gt;Chord P2P hashing&lt;/em&gt; 같은 경우는 &lt;em&gt;Cassandra&lt;/em&gt;, &lt;em&gt;Voldmort&lt;/em&gt; 등의 &lt;em&gt;key-value store&lt;/em&gt; 에서 쓰이고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;napster&#34;&gt;Napster&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week3/napster.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;최초에 &lt;em&gt;peer&lt;/em&gt; 는 서버에게 메세지를 보내 P2P 시스템에 가입했다는 사실을 알립니다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Napster&lt;/em&gt; 에서는 중앙에 서버를 두어, 파일이 저장된 &lt;em&gt;peer&lt;/em&gt; 를 기록합니다. 각 &lt;em&gt;peer&lt;/em&gt; 는 파일이 어디있는지 검색하기 위해 중앙 서버에 질의해야 합니다. 그림에서 볼 수 있듯이, 각 파일은 서버가 아니라 &lt;strong&gt;클라이언트&lt;/strong&gt; 에 저장되어 있습니다. 파일이 어느 클라이언트(&lt;em&gt;peer&lt;/em&gt;) 에 저장되어있는지 알게되면, &lt;em&gt;ping&lt;/em&gt; 을 날려 살아있는지 확인 후 파일을 다운 받습니다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Napster&lt;/em&gt; 의 문제점은&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;중앙 서버로의 요청이 너무나 많습니다.&lt;/li&gt;
&lt;li&gt;서버가 다운되면, 시스템이 멈춥니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;gnutella&#34;&gt;Gnutella&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Gnutella&lt;/em&gt; 는 &lt;em&gt;Napster&lt;/em&gt; 시스템에서 &lt;strong&gt;서버&lt;/strong&gt;를 제거했습니다. 각 클라이언트 (&lt;em&gt;peer&lt;/em&gt;) 는 파일이 어디 저장되어있는지 파악하기 위해 서로 통신하지요. 이처럼 클라이언트가 서버처럼 행동하기때문에 &lt;em&gt;servent&lt;/em&gt; 라 부르기도 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week3/gnutella.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;위 슬라이드에서 알 수 있듯이, 각 피어는 근처에 있는 피어로의 링크를 가지고 있습니다. 이 링크는 &lt;em&gt;overlay graph&lt;/em&gt; 라 부르기도 합니다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;gnutella&lt;/em&gt; 에서 피어간 통신에 사용되는 주요 메세지 타입은&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Query:&lt;/strong&gt; search&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;QueryHit:&lt;/strong&gt; reponse to query&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ping:&lt;/strong&gt; to probe network for other peers&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pong:&lt;/strong&gt; reply to ping, contains address of another peer&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Push:&lt;/strong&gt; used to initiate file transfer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week3/gnutella_header.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week3/gnutella_search_ttl.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림에서 &lt;code&gt;TTL = 2&lt;/code&gt; 이기 때문에 &lt;em&gt;query&lt;/em&gt; 메세지는 &lt;em&gt;2-hop&lt;/em&gt; 까지만 전파됩니다. 그리고 &lt;em&gt;gnutella&lt;/em&gt; 에서는 각 피어가 최근에 퍼트린 &lt;em&gt;query&lt;/em&gt; 메세지 리스트를 유지하고 있기 때문에 같은 메세지를 다시 전파하지 않습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week3/gnutella_queryhit_msg.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week3/gnutella_queryhit_msg_ex.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;피어가 보낸 &lt;em&gt;query&lt;/em&gt; 에 대해 해당하는 파일을 가지고 있다는 응답은 &lt;em&gt;query hit&lt;/em&gt; 메세지를 통해 전달됩니다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;gnutella&lt;/em&gt; 에서는 과도한 트래픽을 방지하기 위해 다음의 방법을 사용합니다.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;to avoid duplicate transmissions, each peer maintains a list of recently received messages&lt;/li&gt;
&lt;li&gt;query forwarded to all neighbors except peer from which received&lt;/li&gt;
&lt;li&gt;each query (identified by &lt;code&gt;DescriptorID&lt;/code&gt;) forwarded only once&lt;/li&gt;
&lt;li&gt;&lt;em&gt;QueryHit&lt;/em&gt; routed back only to peer from which &lt;em&gt;Query&lt;/em&gt; received with same &lt;code&gt;DescriptorID&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;for flooded messages, duplicates with same &lt;code&gt;DescriptorID&lt;/code&gt; and &lt;em&gt;Payload descriptor&lt;/em&gt; are dropped&lt;/li&gt;
&lt;li&gt;&lt;em&gt;QueryHit&lt;/em&gt; with &lt;code&gt;DescriptorID&lt;/code&gt; for which &lt;em&gt;Query&lt;/em&gt; not seen is dropped&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week3/after_receiving_queryhit.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;QueryHit&lt;/em&gt; 메세지를 &lt;em&gt;requestor&lt;/em&gt; 가 받으면 최적의 &lt;em&gt;responder&lt;/em&gt; 를 고르고,  &lt;strong&gt;HTTP&lt;/strong&gt; 를 이용해서 몇번의 통신을 한 뒤 파일을 전송받습니다. 여기서 &lt;em&gt;gnutella&lt;/em&gt; 가 &lt;em&gt;HTTP&lt;/em&gt; 를 이용하는 이유는, HTTP 가 &lt;em&gt;standard&lt;/em&gt;, &lt;em&gt;well-debugged&lt;/em&gt;, &lt;em&gt;widely used&lt;/em&gt; 이기 때문입니다.&lt;/p&gt;

&lt;p&gt;그런데 만약, &lt;em&gt;responder&lt;/em&gt; 가 방화벽(&lt;em&gt;firewall&lt;/em&gt;) 뒤에 있으면 어떻게 될까요? 일반적으로 방화벽은 &lt;em&gt;incomming message&lt;/em&gt; 를 필터링 합니다. &lt;em&gt;gnutella&lt;/em&gt; 는 이럴 경우 대비해 &lt;em&gt;push&lt;/em&gt; 를 만들어 놓았습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week3/dealing_with_firewalls.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;query hit&lt;/em&gt; 메세지를 받은 후에 &lt;em&gt;requestor&lt;/em&gt; 가 보내는 &lt;em&gt;HTTP&lt;/em&gt; 메세지에 &lt;em&gt;responder&lt;/em&gt; 가 응답하지 않으면 &lt;em&gt;overlay link&lt;/em&gt; (이미 연결되어있는) 을 통해서 &lt;em&gt;push&lt;/em&gt; 메세지를 &lt;em&gt;requestor&lt;/em&gt; 가 보냅니다. &lt;em&gt;responder&lt;/em&gt; 는 방화벽 뒤에 있어도, &lt;em&gt;overlay link&lt;/em&gt; 를 통해 받은 &lt;em&gt;push&lt;/em&gt; 메세지를 확인하고 파일 전송을 시작합니다.&lt;/p&gt;

&lt;p&gt;만약 &lt;em&gt;requestor&lt;/em&gt; 가 방화벽 뒤에 있다면, &lt;em&gt;gnutella&lt;/em&gt; 프로토콜로는 파일을 전송 받을 수 없습니다.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;gnutella&lt;/em&gt; 에서 생기는 문제점은&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;ping/pong&lt;/em&gt; constituted 50% traffic: use multiplex, cache and reduce freq of &lt;em&gt;ping/pong&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;modem-conncted hosts do not have enough bandwidth for passing gnutella traffic: use a central server to act as proxy for such peers. or use &lt;strong&gt;FastTrack System&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;large number of &lt;em&gt;free loaders&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;flooding causes excessive traffic: use &lt;strong&gt;Structured P2P system&lt;/strong&gt; e.g &lt;strong&gt;Chord System&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;fasttrac&#34;&gt;FastTrac&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;FastTrac&lt;/em&gt; 은 &lt;em&gt;Kazza&lt;/em&gt;, &lt;em&gt;KazzaLite&lt;/em&gt;, &lt;em&gt;Grokster&lt;/em&gt; 라는 기술을 기반으로 한 &lt;em&gt;Napster&lt;/em&gt; &lt;em&gt;Gnutella&lt;/em&gt; 의 하이브리드입니다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;healthier participants&lt;/em&gt; 를 이용하겠다는 기본적인 아이디어로부터 출발했습니다. &lt;em&gt;gnutella&lt;/em&gt; 와 비슷하지만 노드중 일부가 &lt;em&gt;supernode&lt;/em&gt; 가 되어, 특별한 역할을 수행합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week3/fast_trac.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;*supernode*는 &lt;em&gt;Napster server&lt;/em&gt; 와 비슷하게 근처에 있는 노드의 &lt;code&gt;&amp;lt;file name, peer point&amp;gt;&lt;/code&gt; 리스트를 저장합니다&lt;/li&gt;
&lt;li&gt;&lt;em&gt;supernode&lt;/em&gt; 의 멤버십은 시간이 지나면서 변합니다&lt;/li&gt;
&lt;li&gt;어떤 노드도 &lt;em&gt;supernode&lt;/em&gt; 가 될 수 있습니다. 그러기 위해서는 &lt;em&gt;reputation&lt;/em&gt; 을 얻어야 합니다&lt;/li&gt;
&lt;li&gt;각 노드는 데이터를 탐색하기 위해 &lt;em&gt;supernode&lt;/em&gt; 에 질의합니다&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이 &lt;em&gt;reputation system&lt;/em&gt; 은 &lt;em&gt;Kazaalite&lt;/em&gt; 처럼 &lt;em&gt;upload&lt;/em&gt; 한 파일의 양으로 결정할 수도 있고, 경제학적인 원리를 적용한 방법도 있습니다&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;bittorrent&#34;&gt;BitTorrent&lt;/h3&gt;

&lt;p&gt;이전에 언급했듯이 &lt;strong&gt;다운만 받는 peer&lt;/strong&gt; 도 존재할 수 있습니다. &lt;em&gt;BitTorrent&lt;/em&gt; 는 업로드 하는 &lt;em&gt;peer&lt;/em&gt; 에게 보상을 해 주어, &lt;em&gt;peer&lt;/em&gt; 들의 업로드를 더 이끌어 낼 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week3/bit_torrent_network.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;BitTorrent&lt;/em&gt; 네트워크 구성은 위 슬라이드와 같습니다.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Tracker:&lt;/strong&gt; 파일당 하나씩 존재하며 &lt;em&gt;heartbeat&lt;/em&gt; 를 받아 &lt;em&gt;peer&lt;/em&gt; 의 &lt;em&gt;join&lt;/em&gt;, &lt;em&gt;leave&lt;/em&gt; 를 관리합니다.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Seed:&lt;/strong&gt; 전체 파일을 가지고 있는 &lt;em&gt;peer&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Leecher:&lt;/strong&gt; 파일의 일부분을 가지고 있는 &lt;em&gt;peer&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;BitTorrent&lt;/em&gt; 에서는 블럭단위로 파일을 전송하는데, 이 때 사용하는 몇 가지 규칙이 있습니다.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Local Rarest First:&lt;/em&gt; 파일을 다운받을때, 귀한 블럭부터 먼저 받습니다&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Tit for tat:&lt;/em&gt; 업로드 하는 만큼, 다운로드 &lt;em&gt;bandwidth&lt;/em&gt; 를 할당받습니다. 다시 말해서 업로드를 많이해야 빠르게 받을 수 있습니다&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Choking:&lt;/em&gt; 동시에 업로드하는 &lt;em&gt;neighbor&lt;/em&gt; 수를 제한해서 &lt;em&gt;bandwidth&lt;/em&gt; 가 너무 많이 사용되지 않도록 합니다. &lt;em&gt;best neighbor&lt;/em&gt; 를 선택하여 &lt;em&gt;unchoked set&lt;/em&gt; 을 유지하고, 주기적으로 이 집합을 재평가합니다. 이외의 다른 &lt;em&gt;peer&lt;/em&gt; 는 &lt;em&gt;choked set&lt;/em&gt; 입니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;optimistic unchoke&lt;/em&gt; 기법은 주기적으로 랜덤한 &lt;em&gt;neighbor&lt;/em&gt; 를 &lt;em&gt;unchoke&lt;/em&gt; 해서, &lt;em&gt;unchoked set&lt;/em&gt; 을 &lt;em&gt;fresh&lt;/em&gt; 하게 유지합니다. 여기서 &lt;em&gt;random choice choking&lt;/em&gt; 을 쓰는 이유는&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;To avoid the sysem from getting stuck where only a few peers receive service&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;dht&#34;&gt;DHT&lt;/h3&gt;

&lt;p&gt;지금까지 본 &lt;em&gt;Napster&lt;/em&gt;, &lt;em&gt;Gnutella&lt;/em&gt;, &lt;em&gt;FastTrac&lt;/em&gt; 은 일종의 &lt;em&gt;DHT, Distribute Hash Table&lt;/em&gt; 입니다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;DHT&lt;/em&gt; 에서의 &lt;em&gt;performance concerns&lt;/em&gt; 는&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;load balancing&lt;/li&gt;
&lt;li&gt;fault-tolerance&lt;/li&gt;
&lt;li&gt;efficiency of lookup and inserts&lt;/li&gt;
&lt;li&gt;locality&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;우리가 배울 &lt;em&gt;Chord&lt;/em&gt; 는 이런 구조가 적용된 &lt;em&gt;structured peer to peer system&lt;/em&gt; 입니다.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week3/performance_comparison_nap_gnu.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Napster&lt;/em&gt; 는 &lt;em&gt;peer&lt;/em&gt; 의 경우 파일을 저장하지 않기 때문에 메모리가 많이 들지 않지만, &lt;em&gt;server&lt;/em&gt; 에서 많은 메모리를 요구합니다. 서버로 질의가 가기때문에 &lt;em&gt;lookup latency&lt;/em&gt; 나 &lt;em&gt;lookup&lt;/em&gt; 을 위한 메세지 수 자체는 많지 않지만, 서버의 부하가 상당히 심할 수 있습니다.&lt;/p&gt;

&lt;p&gt;반면 &lt;em&gt;Gnutella&lt;/em&gt; 에서는 서버가 없습니다. 그렇기 때문에 피어는 파일이 저장되어있는 주변 피어의 목록을 가지고 있어야 하는데, &lt;code&gt;N&lt;/code&gt; 만큼의 이웃이 주변에 있을 수 있습니다. 따라서 한 피어에서 필요한 메모리 양은 &lt;code&gt;O(N)&lt;/code&gt; 입니다.&lt;/p&gt;

&lt;p&gt;그리고 네트워크가 직선으로 구성되어 있다고 할때, &lt;em&gt;lookup latency&lt;/em&gt; 는 &lt;code&gt;O(N)&lt;/code&gt; (&lt;code&gt;N-1&lt;/code&gt;) 이고 룩업을 위한 메세지 수도 &lt;code&gt;O(N)&lt;/code&gt; (&lt;code&gt;2(N-1)&lt;/code&gt;) 입니다.&lt;/p&gt;

&lt;p&gt;반면 &lt;em&gt;Chord&lt;/em&gt; 는 모두 &lt;code&gt;O(log N)&lt;/code&gt; 입니다. 이론적으로 &lt;em&gt;constant&lt;/em&gt; 는 아니지만, &lt;em&gt;real world&lt;/em&gt; 에서는 상당히 낮은 수가 될 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;chord&#34;&gt;Chord&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Chord&lt;/em&gt; 는 &lt;em&gt;Berkeley&lt;/em&gt; 와 &lt;em&gt;MIT&lt;/em&gt; 에서 개발된 &lt;em&gt;P2P&lt;/em&gt; 프로토콜입니다.&lt;em&gt;latency&lt;/em&gt; 와 &lt;em&gt;message cost of routing&lt;/em&gt; (&lt;em&gt;lookups&lt;/em&gt;/&lt;em&gt;inserts&lt;/em&gt;) 를 줄이기 위해 지능적으로 &lt;em&gt;neighbor&lt;/em&gt; 를 선택하고 &lt;em&gt;Consistent Hashing&lt;/em&gt; 기법을 사용합니다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Consistent Hasing&lt;/em&gt; 값은 &lt;em&gt;peer&lt;/em&gt; 에 부여되는 주소값으로&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;IP 와 Port로 &lt;em&gt;SHA1&lt;/em&gt; 로 해싱해서 160 비트 스트링을 만들고&lt;/li&gt;
&lt;li&gt;&lt;code&gt;m&lt;/code&gt; 비트로 절단해서 사용합니다&lt;/li&gt;
&lt;li&gt;&lt;em&gt;peer&lt;/em&gt; 의 &lt;em&gt;ID&lt;/em&gt; 라 불리기도 하는데, 이 값은 당연히 최대 &lt;code&gt;2^m - 1&lt;/code&gt; 입니다&lt;/li&gt;
&lt;li&gt;해싱값이므로 &lt;em&gt;unique&lt;/em&gt; 하진 않지만 충돌이 일어날 확률은 굉장히 적습니다&lt;/li&gt;
&lt;li&gt;그리고 이 값이 &lt;code&gt;2^m&lt;/code&gt; 개의 점이 되어 하나의 원을 구성합니다&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week3/ring_of_peers.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week3/chord_finger_table.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;각 노드는 (반) 시계방향으로의 &lt;em&gt;successor&lt;/em&gt; 를 가지고 있고, 다른 노드를 가리키기 위한 &lt;em&gt;finger table&lt;/em&gt; 을 가지고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week3/chord_file_saving.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;파일도 마찬가지로 &lt;em&gt;SHA-1&lt;/em&gt; 으로 해싱해서, 160 비트로 짜른 뒤 &lt;code&gt;mod 2^m&lt;/code&gt; 연산해서, 같은 값이거나 그보다 큰 값을 가지는 &lt;em&gt;peer&lt;/em&gt; 에 저장합니다.&lt;/p&gt;

&lt;p&gt;만약 균일하게 해싱된다면 &lt;code&gt;K&lt;/code&gt; 개의 키, &lt;code&gt;N&lt;/code&gt; 개의 피어에서 파일은 각 피어당 &lt;code&gt;K/N&lt;/code&gt; 개씩 저장되므로 피어당 걸리는 부하는 &lt;code&gt;O(K/N)&lt;/code&gt; 입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week3/chord_search_process.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림에서 &lt;code&gt;N80&lt;/code&gt; 피어가 &lt;code&gt;K42&lt;/code&gt; 파일을 찾을때,&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;finger table&lt;/em&gt; 에 &lt;code&gt;42&lt;/code&gt; 가 없으므로 최대한 먼 &lt;code&gt;N16&lt;/code&gt; 에 질의하고,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;N16&lt;/code&gt; 은 &lt;code&gt;N32&lt;/code&gt; 와 &lt;code&gt;N80&lt;/code&gt; 밖에 모르므로 &lt;code&gt;N32&lt;/code&gt; 를 거쳐 &lt;code&gt;N45&lt;/code&gt; 로 질의합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week3/chord_search_analysis.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;chrod search&lt;/em&gt; 는 &lt;code&gt;O(log N)&lt;/code&gt; 의 시간이 듭니다. 증명에 대한 &lt;em&gt;intuition&lt;/em&gt; 은 쉽습니다.&lt;/p&gt;

&lt;p&gt;만약 현재 &lt;code&gt;Here&lt;/code&gt; 에서 &lt;code&gt;Key&lt;/code&gt; 를 모른다고 합시다. 그러면 그 거리의 &lt;code&gt;1/2&lt;/code&gt; 만큼은 점프를 해야합니다. 그것보다 더 적게 점프하면 거리를 &lt;code&gt;d&lt;/code&gt; 라 합시다. &lt;em&gt;finger table entry&lt;/em&gt; 값은 2배씩 증가하기 때문에, &lt;code&gt;2d&lt;/code&gt; 만큼 점프할 수 있는 엔트리가 있어야 하고, 그럼 애초부터 &lt;code&gt;2d&lt;/code&gt; 만큼 점프했어야 했기 때문에 모순입니다.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;log(N)&lt;/code&gt; 만큼의 점프 뒤에는 &lt;em&gt;key&lt;/em&gt; 까지의 거리는 아무리 멀어봐야 &lt;code&gt;2^m / N&lt;/code&gt; 입니다. 균일하게 분포되는 해싱을 쓴다 가정하면, 이 사이에는 적은 수의 노드만 있습니다. 따라서 &lt;code&gt;O(logN)&lt;/code&gt; 만큼만 더 점프한다면 높은 확률로 &lt;em&gt;key&lt;/em&gt; 를 찾을 수 있습니다. &lt;code&gt;O(logN) + O(logN) = O(logN)&lt;/code&gt; 이므로, &lt;em&gt;search&lt;/em&gt; 는 &lt;code&gt;O(logN)&lt;/code&gt; 입니다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;insertion&lt;/em&gt; 도 &lt;em&gt;searching&lt;/em&gt; 과 마찬가지로 &lt;code&gt;O(logN)&lt;/code&gt; 입니다. 그러나 이 성능은 &lt;em&gt;finger table&lt;/em&gt; 과 &lt;em&gt;successor&lt;/em&gt; 가 잘못되지 않았을 경우에만 참입니다.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week3/chord_multiple_successor.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;chrod&lt;/em&gt; 는 &lt;em&gt;successor&lt;/em&gt; 한개만 가질땐 &lt;em&gt;failure&lt;/em&gt; 에 취약하기 때문에, 위 그림처럼 다수개의 &lt;em&gt;successor&lt;/em&gt; 를 가질 수 있습니다. 이 경우 성능은 어떻게될까요?&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week3/multiple_successor_analysis.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;2log(N)&lt;/code&gt; 개의 &lt;em&gt;successor&lt;/em&gt; 를 유지할 경우를 한번 생각해 봅시다. &lt;code&gt;50%&lt;/code&gt; 씩 &lt;em&gt;failure&lt;/em&gt; 가 발생하면&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;하나의 노드에서 유지하는 &lt;em&gt;successor&lt;/em&gt; 중, 적어도 하나의 &lt;em&gt;successor&lt;/em&gt; 가 살아있을 확률은&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;http://latex.codecogs.com/gif.latex?1%20-%20%28%7B1%20%5Cover%202%7D%29%5E%7B2logN%7D%20%3D%201%20-%20%7B1%20%5Cover%20N%5E2%7D&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;위 확률은 모든 살아있는 노드(&lt;code&gt;50%&lt;/code&gt;) 에서 참일때, 다시 말해서 모든 노드에서 적어도 하나의 &lt;em&gt;successor&lt;/em&gt; 가 존재할 확률은 &lt;code&gt;N&lt;/code&gt; 이 매우 클때&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;http://latex.codecogs.com/gif.latex?%281%20-%20%7B1%20%5Cover%20N%5E2%7D%29%5E%7BN/2%7D%20%3D%20e%5E%7B-%7B1%5Cover%202N%7D%7D%20%5Capprox%201&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week3/chord_joining.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;a new peer affects &lt;code&gt;O(logN)&lt;/code&gt; other finger entires in the system, on average&lt;/li&gt;
&lt;li&gt;number of messages per peer join &lt;code&gt;O(logN * logN)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week3/chord_stabilization_protocol.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;join&lt;/em&gt;, &lt;em&gt;leave&lt;/em&gt;, &lt;em&gt;failure&lt;/em&gt; 등 &lt;em&gt;churn&lt;/em&gt; 이 자주 일어나므로 &lt;em&gt;loop&lt;/em&gt; 가 있는지 없는지 검사하기 위해 주기적으로 &lt;em&gt;stabilization protocol&lt;/em&gt; 를 사용합니다.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;pastry&#34;&gt;Pastry&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week3/pastry_routing.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week3/pastry_locality.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Pastry&lt;/em&gt; 는 &lt;em&gt;chord&lt;/em&gt; 처럼 &lt;em&gt;node&lt;/em&gt; 에 &lt;em&gt;id&lt;/em&gt; 를 부여합니다. &lt;em&gt;routing table&lt;/em&gt; 은 &lt;em&gt;prefix matching&lt;/em&gt; 에 기반하기 때문에 &lt;code&gt;log(N)&lt;/code&gt; 의 성능을 보여줍니다. 그리고 짧은 &lt;em&gt;prefix&lt;/em&gt; 일 수록 가까이에 있을 확률이 높습니다.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;kelips&#34;&gt;Kelips&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week3/kelips.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week3/kelips2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week3/kelips3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Kelips&lt;/em&gt; 는 &lt;em&gt;1-hop lookup&lt;/em&gt; 을 보여줍니다. 이럴 수 있는 이유는 위 그림에서 보듯이 &lt;em&gt;affinity group&lt;/em&gt; 이란걸 사용하기 때문입니다. 루트 &lt;code&gt;N&lt;/code&gt; 에 가까운 숫자 &lt;code&gt;k&lt;/code&gt; 를 정하고, 이 수로 &lt;code&gt;mod&lt;/code&gt; 연산을 해, 그룹을 만듭니다. 각각의 그룹은 내에 있는 모든 노드는 서로 어떤 파일을 저장하는지 알고 있습니다. 그리고 각 노드는 다른 그룹으로의 링크를 하나씩 가지고 있습니다. 따라서 어딜가든 거의 1번 혹은 2번 내에 &lt;em&gt;lookup&lt;/em&gt; 이 가능합니다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;chord&lt;/em&gt; 에 비해 메모리를 더 잡아먹긴 합니다. &lt;code&gt;O(logN)&lt;/code&gt; 보단 많은 양이지만, 그렇게 많지도 않습니다. 메모리가 귀하다면 &lt;em&gt;chord&lt;/em&gt; 나 &lt;em&gt;pastry&lt;/em&gt; 를, 그렇지 않고 &lt;em&gt;lookup&lt;/em&gt; 속도가 중요하다면 &lt;em&gt;kelips&lt;/em&gt; 를 사용하면 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week3/kelips4.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;membership&lt;/em&gt; 은 &lt;em&gt;gossip-based&lt;/em&gt; 프로토콜로 관리할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;refs&#34;&gt;Refs&lt;/h3&gt;

&lt;p&gt;(1) &lt;a href=&#34;http://ook.co/solutions/cloud-computing/&#34;&gt;Title Image&lt;/a&gt;&lt;br /&gt;
(2) &lt;strong&gt;Cloud Computing Concept 1&lt;/strong&gt; by &lt;em&gt;Indranil Gupta&lt;/em&gt;, Coursera&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CC 05: Global Snapshot</title>
      <link>https://1ambda.github.io/93/cloud-computing/cloud-computing-5/</link>
      <pubDate>Sat, 25 Jun 2016 14:42:45 +0900</pubDate>
      
      <guid>https://1ambda.github.io/93/cloud-computing/cloud-computing-5/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://ook.co/wp-content/uploads/cloudcomputing.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;이번시간에는 &lt;em&gt;Distributed Snapshot&lt;/em&gt; 에 대해서 배웁니다. 클라우드 환경에서 각 어플리케이션(혹은 서비스) 는 여러개의 서버 위에서 돌아갑니다. 각 서버는 &lt;em&gt;concurrent events&lt;/em&gt; 를 다루며, 서로 상호작용합니다. 이런 환경에서 &lt;em&gt;global snapshot&lt;/em&gt; 을 캡쳐할 수 있다면&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;check pointing:&lt;/strong&gt; can restart distributed application on failure&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;garbage collection of objects:&lt;/strong&gt; object at servers that don&amp;rsquo;t have any other objects(ay any servers) with pointers to them&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;deadlock detection:&lt;/strong&gt; useful in database transaction systems&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;termination of computation:&lt;/strong&gt; useful in batch computing systems like Folding@Homes, SETI@Home&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;global snapshot&lt;/em&gt; 은 두 가지를 포함합니다.&lt;/p&gt;

&lt;p&gt;(1) Individual state of each process
(2) Individual state of each communication channel&lt;/p&gt;

&lt;p&gt;&lt;em&gt;global snapshot&lt;/em&gt; 을 만드는 한가지 방법은 모든 프로세스의 &lt;em&gt;clock&lt;/em&gt; 을 동기화 하는 것입니다. 그래서 모든 프로세스에게 &lt;em&gt;time&lt;/em&gt; &lt;code&gt;t&lt;/code&gt; 에서의 자신의 상태를 기록하도록 요구할 수 있습니다. 그러나&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Time synchorization always has error&lt;/li&gt;
&lt;li&gt;Doesn&amp;rsquo;t not record the state of meesages in the channels&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;지난 시간에 보았듯이, &lt;em&gt;synchronization&lt;/em&gt; 이 아니라 &lt;em&gt;casuality&lt;/em&gt; 로도 충분합니다. 프로세스가 &lt;strong&gt;명령을 실행하거나&lt;/strong&gt;, &lt;strong&gt;메시지를 받거나&lt;/strong&gt;, &lt;strong&gt;메시지를 보낼때마다&lt;/strong&gt; &lt;em&gt;global system&lt;/em&gt; 가 변합니다. 이를 저장하기 위해서 &lt;em&gt;casuality&lt;/em&gt; 를 기록하는 방법을 알아보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;chandy-lamport-algorithm&#34;&gt;Chandy-Lamport Algorithm&lt;/h3&gt;

&lt;p&gt;시작 전에 &lt;em&gt;system model&lt;/em&gt; 을 정의하면&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;N Processes in the system&lt;/li&gt;
&lt;li&gt;There are two uni-directional communication channels between each ordered process pair &lt;code&gt;P_j -&amp;gt; P_i&lt;/code&gt;, &lt;code&gt;P_i -&amp;gt; P_j&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;communication channels are &lt;strong&gt;FIFO&lt;/strong&gt; ordered&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;No failure&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;All messages arribe intact, and are not duplicated&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;requirements&lt;/em&gt; 는&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;snapshot&lt;/em&gt; 때문에 &lt;em&gt;application&lt;/em&gt; 의 작업에 방해가 일어나서는 안됩니다&lt;/li&gt;
&lt;li&gt;각 프로세스는 자신의 &lt;em&gt;state&lt;/em&gt; 를 저장할 수 있어야 합니다&lt;/li&gt;
&lt;li&gt;&lt;em&gt;global state&lt;/em&gt; 는 분산회되어 저장됩니다 (collected in a distributed manner)&lt;/li&gt;
&lt;li&gt;어떤 프로세스든지, &lt;em&gt;snapshot&lt;/em&gt; 작업을 시작할 수 있습니다&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/Chandy_Lamport1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;프로세스 &lt;code&gt;P_i&lt;/code&gt; 가 &lt;em&gt;market&lt;/em&gt; 메세지를 만들고, 자신을 제외한 다른 &lt;code&gt;N-1&lt;/code&gt; 개의 프로세스에게 보냅니다&lt;/li&gt;
&lt;li&gt;동시에 &lt;code&gt;P_i&lt;/code&gt; 는 &lt;em&gt;incoming channel&lt;/em&gt; 을 레코딩하기 시작합니다&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/Chandy_Lamport2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;(1) 만약 &lt;code&gt;P_i&lt;/code&gt; 가 &lt;em&gt;marker&lt;/em&gt; 메시지를 처음 받는다면&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;만약메시지를 받은 프로세스 &lt;code&gt;P_i&lt;/code&gt; 에서는 자신의 &lt;em&gt;state&lt;/em&gt; 를 기록하고&lt;/li&gt;
&lt;li&gt;자신을 제외한 프로세스들에게 &lt;em&gt;marker&lt;/em&gt; 보내고&lt;/li&gt;
&lt;li&gt;는 &lt;em&gt;incoming channel&lt;/em&gt; 을 레코딩하기 시작합니다&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(2) &lt;code&gt;P_i&lt;/code&gt; 가 이미 &lt;em&gt;market&lt;/em&gt; 메세지를 받은적이 있다면&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;이미 해당 채널의 모든 메세지를 기록중이었으므로, 레코딩을 끝냅니다&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/Chandy_Lamport3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;이 알고리즘은 모든 프로세스가 자신의 &lt;em&gt;state&lt;/em&gt; 와 모든 &lt;em&gt;channel&lt;/em&gt; 을 저장하면 종료됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/Chandy_Lamport_Example1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/Chandy_Lamport_Example2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/Chandy_Lamport_Example3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/Chandy_Lamport_Example4.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/Chandy_Lamport_Example5.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/Chandy_Lamport_Example6.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;consistent-cuts&#34;&gt;Consistent Cuts&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Chandy-Lamport&lt;/em&gt; 알고리즘은 &lt;em&gt;casuality&lt;/em&gt; 를 보장합니다. 이에 대해 증명하기 전에 먼저, &lt;em&gt;consistent cut&lt;/em&gt; 이란 개념을 보고 가겠습니다.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Cut:&lt;/strong&gt; time frontier at each process and at each channel. Events at the process/channel that happen before the cut are &lt;strong&gt;in the cut&lt;/strong&gt; and happening after the cut are &lt;strong&gt;out of the cut&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Consistent Cut:&lt;/strong&gt; a cut that obeys casuality. A cut &lt;code&gt;C&lt;/code&gt; is a consistent cut iff for each pair of event &lt;code&gt;e&lt;/code&gt; &lt;code&gt;f&lt;/code&gt; in the system, such that event &lt;code&gt;e&lt;/code&gt; is in the cur &lt;code&gt;C&lt;/code&gt; and if &lt;code&gt;f -&amp;gt; e&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;다시 말해서 &lt;code&gt;e&lt;/code&gt; 가 &lt;code&gt;C&lt;/code&gt; 내에 있고, &lt;code&gt;f -&amp;gt; e&lt;/code&gt; 라면 &lt;code&gt;f&lt;/code&gt; 도 &lt;code&gt;C&lt;/code&gt; 에 있어야만 &lt;em&gt;consistent cut&lt;/em&gt; 이란 뜻입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/consistent_cut1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;F&lt;/code&gt; 가 &lt;em&gt;cut&lt;/em&gt; 내에 있지만, 올바르게 캡쳐되어 메시지 큐 내에서 전송중임을 &lt;em&gt;snapshot&lt;/em&gt; 에서 보장합니다. 하지만 &lt;code&gt;G -&amp;gt; D&lt;/code&gt; 같은 경우는, &lt;code&gt;D&lt;/code&gt; 가 &lt;em&gt;cut&lt;/em&gt; 내에 있지만 &lt;code&gt;G&lt;/code&gt; 가 그렇지 않아 &lt;em&gt;inconsistent cut&lt;/em&gt; 입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/consistent_cut2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Chandy-Lamport Global Snapshot&lt;/em&gt; 알고리즘은 항상 &lt;em&gt;consistent cut&lt;/em&gt; 을 만듭니다. 왜 그런가 증명을 보면&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/Chandy_Lamport_Proof1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ei -&amp;gt; ej&lt;/code&gt; 를 보장한다는 말은 스냅샷 안에 두 이벤트가 있다는 뜻입니다. 따라서 &lt;code&gt;ej -&amp;gt; &amp;lt;P_j records its state&amp;gt;&lt;/code&gt; 일때 당연히 &lt;code&gt;ei -&amp;gt; &amp;lt;P_i records its state&amp;gt;&lt;/code&gt; 와 같은 말입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/Chandy_Lamport_Proof2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;만약 &lt;code&gt;ej -&amp;gt; &amp;lt;P_j records its state&amp;gt;&lt;/code&gt; 일때 &lt;code&gt;&amp;lt;P_i records its state&amp;gt; -&amp;gt; ei&lt;/code&gt; 라 합시다.&lt;/p&gt;

&lt;p&gt;그러면 &lt;code&gt;ei -&amp;gt; ej&lt;/code&gt; 로 가는 &lt;em&gt;regular app message&lt;/em&gt; 경로를 생각해 봤을때, &lt;code&gt;P_i&lt;/code&gt; 가 먼저 자신의 상태를 기록하기 시작했으므로 &lt;em&gt;marker&lt;/em&gt; 메세지가 먼저 날라갈겁니다. (FIFO) 그러면 위에서 말한 &lt;code&gt;ei -&amp;gt; ej&lt;/code&gt; 경로를 타고 &lt;em&gt;marker&lt;/em&gt; 메세지가 먼저 가게되고 &lt;code&gt;P_j&lt;/code&gt; 는 자신의 상태를 먼저 기록하게 됩니다. 따라서 &lt;code&gt;P_j&lt;/code&gt; 에서 &lt;code&gt;ej&lt;/code&gt; 보다 자신의 상태를 기록하는 것이 먼저이므로 &lt;code&gt;ej&lt;/code&gt; 는 &lt;em&gt;out of cut&lt;/em&gt; 이고, 모순입니다.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;safety-and-liveness&#34;&gt;Safety and Liveness&lt;/h3&gt;

&lt;p&gt;분산시스템의 &lt;em&gt;correctness&lt;/em&gt; 와 관련해서 &lt;em&gt;safety&lt;/em&gt; 와 &lt;em&gt;liveness&lt;/em&gt; 란 개념이 있습니다. 이 둘은 주로 혼동되어 사용되는데, 둘을 구별하는 것은 매우 중요합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/liveness.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;distributed computation will terminate eventually&lt;/li&gt;
&lt;li&gt;every failure is eventually deteced by some non-faulty process&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/safety.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;there is no deadlock in a distributed transaction system&lt;/li&gt;
&lt;li&gt;no object is orphaned&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;accuracy&lt;/strong&gt; in failure detector&lt;/li&gt;
&lt;li&gt;no two processes decide on different values&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/liveness_and_safety.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;failure detector&lt;/em&gt; 나 &lt;em&gt;concensus&lt;/em&gt; 의 경우에서 볼 수 있듯이 &lt;em&gt;completeness&lt;/em&gt; 와 &lt;em&gt;accuracy&lt;/em&gt; 두 가지를 모두 충족하긴 힘듭니다.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/language_of_global_state.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;global snapshot&lt;/em&gt; 은 한 상태 &lt;code&gt;S&lt;/code&gt; 이고, 여기서 다른 스냅샷으로의 이동은 &lt;em&gt;casual step&lt;/em&gt; 을 따라 이동하는 것입니다. 따라서 &lt;em&gt;liveness&lt;/em&gt; 와, &lt;em&gt;safety&lt;/em&gt; 와 관련해 다음과 같은 특징이 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/using_global_snapshot.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Chandy-Lamport&lt;/em&gt; 알고리즘은 &lt;em&gt;stable&lt;/em&gt; 한지를 검사하기 위해 사용할 수도 있습니다. 여기서 &lt;em&gt;stable&lt;/em&gt; 하다는 것은, 한번 참이면 그 이후에는 계속 참인 것을 말합니다. 이는 알고리즘이 &lt;em&gt;casual correctness&lt;/em&gt; 를 가지기 때문입니다.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;refs&#34;&gt;Refs&lt;/h3&gt;

&lt;p&gt;(1) &lt;a href=&#34;http://ook.co/solutions/cloud-computing/&#34;&gt;Title Image&lt;/a&gt;&lt;br /&gt;
(2) &lt;strong&gt;Cloud Computing Concept 1&lt;/strong&gt; by &lt;em&gt;Indranil Gupta&lt;/em&gt;, Coursera&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CC 06: Multicast</title>
      <link>https://1ambda.github.io/93/cloud-computing/cloud-computing-6/</link>
      <pubDate>Sat, 25 Jun 2016 14:42:46 +0900</pubDate>
      
      <guid>https://1ambda.github.io/93/cloud-computing/cloud-computing-6/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://ook.co/wp-content/uploads/cloudcomputing.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;multicast&lt;/em&gt; 는 클라우드 시스템에서 많이 사용됩니다. &lt;em&gt;Cassandra&lt;/em&gt; 같은 분산 스토리지에서는 &lt;em&gt;write/read&lt;/em&gt; 메세지를 &lt;em&gt;replica gorup&lt;/em&gt; 으로 보내기도 하고, &lt;em&gt;membership&lt;/em&gt; 을 관리하기 위해서 사용하기도 합니다&lt;/p&gt;

&lt;p&gt;그런데, 이 &lt;em&gt;multicast&lt;/em&gt; 는 &lt;em&gt;ordering&lt;/em&gt; 에 따라서 &lt;em&gt;correctness&lt;/em&gt; 에 영향을 줄 수 있기 때문에 매우 중요합니다. 자주 쓰이는 기법으로 &lt;em&gt;FIFO&lt;/em&gt;, &lt;em&gt;Casual&lt;/em&gt;, &lt;em&gt;Total&lt;/em&gt; 이 있는데 하나씩 살펴보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;ordering&#34;&gt;Ordering&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/Ordering_FIFO.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;FIFO&lt;/em&gt; 를 이용한다면, 보낸 순서대로 도착하게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/Ordering_casual.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/Ordering_casual_example.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;casual ordering&lt;/em&gt; 에서는 반드시 &lt;em&gt;casuality-obeying order&lt;/em&gt; 로 전달해야 합니다. 예를 들어 위 그림에서는 &lt;code&gt;M1:1 -&amp;gt; M3:1&lt;/code&gt; 이기 때문에 반드시 그 순서대로 받아야 합니다. &lt;em&gt;concurrent event&lt;/em&gt; 는 어떤 순서로 받아도 상관 없습니다.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;casual ordering&lt;/em&gt; 이면 &lt;em&gt;FIFO ordering&lt;/em&gt; 입니다. 왜냐하면 같은 프로세스에서 보낸 &lt;em&gt;casuality&lt;/em&gt; 를 따르면 그게 바로 &lt;em&gt;FIFO&lt;/em&gt; 이기 때문입니다. 역은 성립하지 않습니다.&lt;/p&gt;

&lt;p&gt;일반적으로는 &lt;em&gt;casual ordering&lt;/em&gt; 을 사용합니다. 서로 다른 친구로부터 댓글이 달렸는데, 늦게 달린 친구의 댓글이 먼저 보인다면 당연히 말이 되지 않습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/Ordering_total.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/Ordering_total_example.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;total ordering&lt;/em&gt; 은 &lt;em&gt;atomic broadcast&lt;/em&gt; 라 부르는데, 모든 프로세스가 같은 순서로 메시지를 받는것을 보장합니다.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Since &lt;em&gt;FIFO/Casual&lt;/em&gt; are orthogonal to &lt;em&gt;Total&lt;/em&gt;, can have hybrid ordering protocol too (e.g &lt;em&gt;FIFO-total&lt;/em&gt;, &lt;em&gt;Casual-total&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;fifo-ordering-impl&#34;&gt;FIFO Ordering Impl&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/FIFO_impl1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/FIFO_impl2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;각 프로세스는 &lt;em&gt;seq number&lt;/em&gt; 로 구성된 벡터를 유지하고,&lt;/li&gt;
&lt;li&gt;프로세스에서 메시지를 보낼때 마다 자신의 &lt;em&gt;seq number&lt;/em&gt; 를 하나 증가 시켜서 보냅니다&lt;/li&gt;
&lt;li&gt;메시지를 받았을때, &lt;strong&gt;자신의 벡터 내에 있는 값 + 1&lt;/strong&gt; 일 경우에만 벡터 값을 +1 한뒤 전달하고, 아니면 +1 인 값이 올 때까지 버퍼에 넣고 기다립니다&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;예제를 보면&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/FIFO_impl_example.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/FIFO_impl_example2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;total-ordering-impl&#34;&gt;Total Ordering Impl&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/total_impl1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;sequencer-based approach&lt;/em&gt; 입니다. 먼저 하나의 프로세스가 &lt;em&gt;sequencer&lt;/em&gt; 로 선출된 뒤, 어떤 프로세스가 메세지를 보낼때마다 그룹 뿐만 아니라 &lt;em&gt;sequencer&lt;/em&gt; 에게 보내게 됩니다.&lt;/p&gt;

&lt;p&gt;이 &lt;em&gt;sequencer&lt;/em&gt; 는 글로벌 시퀀스 &lt;code&gt;S&lt;/code&gt; 를 유지하면서, 메시지 &lt;code&gt;M&lt;/code&gt; 을 받을때마다 &lt;code&gt;S++&lt;/code&gt; 해서 &lt;code&gt;&amp;lt;M, S&amp;gt;&lt;/code&gt; 로 멀티캐스트를 보냅니다.&lt;/p&gt;

&lt;p&gt;각 프로세스에서는 &lt;em&gt;local&lt;/em&gt; 에 글로벌 시퀀스 &lt;code&gt;Si&lt;/code&gt; 를 유지합니다. 만약 프로세스가 메세지를 받는다면 &lt;code&gt;Si + 1 = S(M)&lt;/code&gt; 값을 글로벌 시퀀서로부터 받을때까지 기다리고, 받은 후에야 &lt;code&gt;Si++&lt;/code&gt; 하고 전달합니다.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;casual-ordering-impl&#34;&gt;Casual Ordering Impl&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/casual_impl1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/casual_impl2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;자료구조 자체는 같으나, &lt;em&gt;casuality&lt;/em&gt; 를 검사하기 위해 &lt;em&gt;sender&lt;/em&gt; 가 &lt;em&gt;vector&lt;/em&gt; 전체를 보냅니다. &lt;em&gt;receiver&lt;/em&gt; 는 메세지를 받으면 다음 두 조건을 만족하기 전까지 버퍼에 넣습니다&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;M[j]&lt;/code&gt; = &lt;code&gt;P_i[j] + 1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;M[k]&lt;/code&gt; &amp;lt;= &lt;code&gt;P_i[k]&lt;/code&gt;, (&lt;code&gt;k != j&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;두번째 조건을 해석하면, 자신의 벡터도 다음 프로세스에게 전달해야 하기 때문에 &lt;code&gt;M[k]&lt;/code&gt; 이후의 벡터만 가지고 있어야 전달할 수 있다는 뜻입니다. (&lt;code&gt;M[j]&lt;/code&gt; 는 제외)&lt;/p&gt;

&lt;p&gt;이 두 조건이 만족되야만 &lt;code&gt;P_i[j] = M[j]&lt;/code&gt; 로 세팅하고 &lt;code&gt;M&lt;/code&gt; 을 전달합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/casual_impl_example1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/casual_impl_example2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;reliable-multicast&#34;&gt;Reliable Multicast&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;reliable&lt;/em&gt; 이란, 루즈하게 말하자면 모든 &lt;em&gt;receiver&lt;/em&gt; 가 메세지를 받는다는 뜻입니다. &lt;em&gt;ordering&lt;/em&gt; 과는 &lt;em&gt;orthogonal&lt;/em&gt; 하기 때문에 &lt;em&gt;Reliable-FIFO&lt;/em&gt;, 등등 구현이 가능합니다. 더 엄밀한 정의는&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;need all &lt;strong&gt;correct&lt;/strong&gt; (&lt;em&gt;non-faulty&lt;/em&gt;) processes to receive the same set of multicasts as all other correct processes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/reliable_multicast_impl1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;단순히 &lt;em&gt;reliable unicast&lt;/em&gt; 를 여러개 보내는것 만으로는 부족합니다. 왜냐하면 &lt;em&gt;sender&lt;/em&gt; 에서 &lt;em&gt;failure&lt;/em&gt; 가 일어날 수 있기 때문입니다&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/reliable_multicast_impl2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/reliable_multicast_impl3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;비효율적이지만, &lt;em&gt;reliable&lt;/em&gt; 합니다.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;virtual-synchrony&#34;&gt;Virtual Synchrony&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;virtual sinchrony&lt;/em&gt; 혹은 &lt;em&gt;view synchrony&lt;/em&gt; 라 불리는데, 이것은 &lt;em&gt;failure&lt;/em&gt; 에도 불구하고 &lt;em&gt;multicast ordering&lt;/em&gt; 과 &lt;em&gt;reliability&lt;/em&gt; 를 얻기 위해 &lt;em&gt;membership protocol&lt;/em&gt; 을 &lt;em&gt;multicast protocol&lt;/em&gt; 과 같이 사용합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/view.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;각 프로세스가 관리하는 &lt;em&gt;membership list&lt;/em&gt; 를 &lt;em&gt;view&lt;/em&gt; 라 부릅니다. &lt;em&gt;virtual synchrony&lt;/em&gt; 프로토콜은 이런 &lt;em&gt;view change&lt;/em&gt; 가 &lt;em&gt;correct process&lt;/em&gt; 에 올바른 순서대로 전달됨을 보장합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/vsync_multicast.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Virtual Synchrony&lt;/em&gt; 프로토콜은 다음을 보장합니다.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the set of multicasts delivered in a given view is the same set at all correct processes that were in that view&lt;/li&gt;
&lt;li&gt;the sender of the multicast message also belongs to that view&lt;/li&gt;
&lt;li&gt;if a process &lt;code&gt;P_i&lt;/code&gt; doesn&amp;rsquo;t not deliver a multicast &lt;code&gt;M&lt;/code&gt; in view &lt;code&gt;V&lt;/code&gt; while other processes in the view &lt;code&gt;V&lt;/code&gt; delivered &lt;code&gt;M&lt;/code&gt; in &lt;code&gt;V&lt;/code&gt;, then &lt;code&gt;P_i&lt;/code&gt; will be &lt;strong&gt;forcibly removed&lt;/strong&gt; from the next view delivered after &lt;code&gt;V&lt;/code&gt; at the other processes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;다시 말해서, &lt;em&gt;multicast&lt;/em&gt; 메세지는 같이 전달된 &lt;em&gt;view&lt;/em&gt; 내에 있던 다른 프로세스에서 모두 동일합니다. 그리고 &lt;em&gt;view&lt;/em&gt; &lt;code&gt;V&lt;/code&gt; 내에 있는 어떤 프로세스가 &lt;code&gt;M&lt;/code&gt; 을 전달하지 못할 경우, 다른 프로세스의 &lt;em&gt;next view&lt;/em&gt; 에서 제거됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/virtual_synchrony_example1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/virtual_synchrony_example2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/virtual_synchrony_example3.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/virtual_synchrony_example4.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/virtual_synchrony_example5.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/virtual_synchrony_example6.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/virtual_synchrony_example7.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/virtual_synchrony_example8.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Called &lt;strong&gt;&amp;ldquo;virtual synchrony&amp;rdquo;&lt;/strong&gt; since in spite of running on an asynchronous network, it gives the appearance of a synchronous network underneath that obeys the same ordering at all processes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그러나 &lt;em&gt;consensus&lt;/em&gt; 를 구현하는데는 쓸 수 없습니다. &lt;em&gt;partitioning&lt;/em&gt; 에 취약하기 때문입니다.&lt;/p&gt;

&lt;p&gt;정리하자면 &lt;em&gt;multicast&lt;/em&gt; 는 클라우드 시스템에서 중요한 요소입니다. 필요에 따라서 &lt;em&gt;ordering&lt;/em&gt;, &lt;em&gt;reliability&lt;/em&gt;, &lt;em&gt;virtual synchorny&lt;/em&gt; 를 구현할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;refs&#34;&gt;Refs&lt;/h3&gt;

&lt;p&gt;(1) &lt;a href=&#34;http://ook.co/solutions/cloud-computing/&#34;&gt;Title Image&lt;/a&gt;&lt;br /&gt;
(2) &lt;strong&gt;Cloud Computing Concept 1&lt;/strong&gt; by &lt;em&gt;Indranil Gupta&lt;/em&gt;, Coursera&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CC 07: Paxos</title>
      <link>https://1ambda.github.io/93/cloud-computing/cloud-computing-7/</link>
      <pubDate>Sat, 25 Jun 2016 14:42:47 +0900</pubDate>
      
      <guid>https://1ambda.github.io/93/cloud-computing/cloud-computing-7/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://ook.co/wp-content/uploads/cloudcomputing.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;대부분의 분산 서버 벤더들은 &lt;code&gt;99.99999%&lt;/code&gt; 의 &lt;em&gt;reliability&lt;/em&gt; 를 보장하지만, &lt;code&gt;100%&lt;/code&gt;는 아닙니다. 왜그럴까요? 그들이 못해서가 아니라 &lt;em&gt;consensus&lt;/em&gt; 문제 때문입니다.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The fault lies in the impossibility of consensus&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;em&gt;Consensus&lt;/em&gt; 문제가 중요한 이유는, 많은 분산 시스템이 &lt;em&gt;consensus&lt;/em&gt; 문제이기 때문입니다.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Perfect Failure Detection&lt;/li&gt;
&lt;li&gt;Leader Election&lt;/li&gt;
&lt;li&gt;Agreement (harder than consensus)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;일반적으로 서버가 많으면 다음의 일들을 해야합니다.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Reliable Multicast:&lt;/strong&gt; Make sure that all of them receive the same updates in the same order as each other&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Membership/Failure Detection:&lt;/strong&gt; To keep their own local lists where they know about each other, and when anyone leaves or fails, everyone is updated simultaneously&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Leader Election:&lt;/strong&gt; Elect a leader among them, and let everyone in the group know about it&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Mutual Exclusion:&lt;/strong&gt; To ensure mutually exclusive access to a critical resource like a file&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이 문제들은 대부분 &lt;em&gt;consensus&lt;/em&gt; 와 연관되어 있습니다. 더 직접적으로 연관되어 있는 문제들은&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The ordering of messages&lt;/li&gt;
&lt;li&gt;The up/down status of a suspected failed process&lt;/li&gt;
&lt;li&gt;Who the leader is&lt;/li&gt;
&lt;li&gt;Who has access to the critical resource&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;consensus-problem&#34;&gt;Consensus Problem&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/consensus_problem.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/consensus_problem2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;모든 프로세스(노드, 서버)가 같은 &lt;em&gt;value&lt;/em&gt; 를 만들도록 해야 하는데, 몇 가지 제약조건이 있습니다.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;validity:&lt;/strong&gt; if everyone propose same value, then that&amp;rsquo;s what&amp;rsquo;s decided&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;integrity:&lt;/strong&gt; decided value must have been proposed by some process&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;non-triviality:&lt;/strong&gt; there is at least one initial system state that leads to each of the all-&lt;code&gt;0&lt;/code&gt;&amp;rsquo;s or all-&lt;code&gt;1&lt;/code&gt;&amp;rsquo;s outcomes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;non-triviality&lt;/em&gt; 는 쉽게 말해서, 모두 &lt;code&gt;0&lt;/code&gt; 이거나 모두 &lt;code&gt;1&lt;/code&gt; 일 수 있는 상태가 있어야 한다는 뜻입니다. 왜냐하면 항상 &lt;code&gt;0&lt;/code&gt; 이거나 &lt;code&gt;1&lt;/code&gt; 만 나오면 &lt;em&gt;trivial&lt;/em&gt; 하기 때문입니다. 별 의미가 없죠.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;models&#34;&gt;Models&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;consensus&lt;/em&gt; 문제는 분산 시스템 모델에 따라 달라집니다. 모델은 크게 2가지로 나눌 수 있는데&lt;/p&gt;

&lt;p&gt;(1) Synchronous Distributed System Model&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Each message is received within bounded time&lt;/li&gt;
&lt;li&gt;Drift of each process&amp;rsquo; local clock has a known bound&lt;/li&gt;
&lt;li&gt;Each step in a process takes &lt;code&gt;lb &amp;lt; time &amp;lt; ub&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;동기 시스템 모델에서는 &lt;em&gt;consensus&lt;/em&gt; 문제를 풀 수 있습니다.&lt;/p&gt;

&lt;p&gt;(2) Asynchronous Distributed System Model&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Nobounds on process execution&lt;/li&gt;
&lt;li&gt;The drift rate of a clock is arbitrary&lt;/li&gt;
&lt;li&gt;No bounds on message transmission delay&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;일반적으로 비동기 분산 시스템 모델이 더 일반적입니다, 그리고 더 어렵죠. 비동기를 위한 프로토콜은 동기 모델 위에서 작동할 수도 있으나, 그 역은 잘 성립하지 않습니다.&lt;/p&gt;

&lt;p&gt;비동기 분산 시스템 모델에서는 &lt;em&gt;consensus&lt;/em&gt; 문제는 풀 수 &lt;strong&gt;없습니다&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Whatever protocol/algorithm you suggest, there is always a worst-case possible execution with failures and message delays that prevens the system from reaching consensus&lt;/li&gt;
&lt;li&gt;Powerful result(see the &lt;strong&gt;FLP&lt;/strong&gt; proof)&lt;/li&gt;
&lt;li&gt;Subsequently, safe and &lt;strong&gt;probabilistic&lt;/strong&gt; solution have become popular (e.g Paxos)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;paxos-in-syncronous-systems&#34;&gt;Paxos in Syncronous Systems&lt;/h3&gt;

&lt;p&gt;동기 시스템이라 가정합니다. 따라서&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;bounds on message dealy&lt;/li&gt;
&lt;li&gt;bounds on upper bound on clock drift rates&lt;/li&gt;
&lt;li&gt;bounds on max time for each process step&lt;/li&gt;
&lt;li&gt;processes can fail by stopping&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/consensus_in_sync_system.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;아무리 많아야 &lt;code&gt;f&lt;/code&gt; 개의 프로세서에서 &lt;em&gt;crash&lt;/em&gt; 가 나고&lt;/li&gt;
&lt;li&gt;모든 프로세서는 &lt;em&gt;round&lt;/em&gt; 단위로 동기화 되고, 동작하며&lt;/li&gt;
&lt;li&gt;&lt;em&gt;reliable communication&lt;/em&gt; 을 통해 서로 통신합니다&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;value_i^r&lt;/em&gt; 을 &lt;em&gt;round&lt;/em&gt; &lt;code&gt;r&lt;/code&gt; 의 시작에 &lt;code&gt;P_i&lt;/code&gt; 에게 알려진 &lt;em&gt;value&lt;/em&gt; 의 집합이라 라 하겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/paxos1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/paxos2.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;f+1&lt;/code&gt; 라운드 후에 모든 &lt;em&gt;correct&lt;/em&gt; 프로세스는 같은 값의 집합을 가지게 되는데, 귀류법으로 쉽게 증명할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/consensus_in_async.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;비동기 환경에서는, 아주아주아주아주아주 느린 프로세서와 &lt;em&gt;failed&lt;/em&gt; 프로세서를 구분할 수 없기 때문에, 나머지 프로세서들이 이것을 결정하기 위해 영원히 기다려야 할지도 모릅니다. 이것이 기본적인 &lt;em&gt;FLP Proof&lt;/em&gt; 의 아이디어입니다. 그렇다면, &lt;em&gt;consensus&lt;/em&gt; 문제를 정말 풀기는 불가능한걸까요?&lt;/p&gt;

&lt;p&gt;풀 수 있습니다. 널리 알려진 &lt;em&gt;consensus-solving&lt;/em&gt; 알고리즘이 있습니다. 실제로는 불가능한 &lt;em&gt;consensus&lt;/em&gt; 문제를 풀려는 것이 아니라, &lt;em&gt;safety&lt;/em&gt; 와 &lt;em&gt;eventual liveness&lt;/em&gt; 를 제공합니다. 야후의 &lt;em&gt;zookeeper&lt;/em&gt; 나 구글의 &lt;em&gt;chubby&lt;/em&gt; 등이 이 알고리즘을 이용합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/yes_we_can_with_paxos.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;safety&lt;/em&gt; 는 서로 다른 두개의 프로세서가 다른 값을 제출하지 않는것을 보장하고, (&lt;em&gt;No two non-faulty processes decide different values&lt;/em&gt;) &lt;em&gt;eventual liveness&lt;/em&gt; 는 운이 좋다면 언젠가는 합의에 도달한다는 것을 말합니다. 근데 실제로는 꽤 빨리 &lt;em&gt;consensus&lt;/em&gt; 문제를 풀 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/paxos_simple.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;본래는 최적화때문에 더 복잡한데, 위 슬라이드에서는 간략화된 &lt;em&gt;paxos&lt;/em&gt; 가 나와있습니다. &lt;em&gt;paxos&lt;/em&gt; 의 &lt;em&gt;round&lt;/em&gt; 마다 고유한 &lt;em&gt;ballot id&lt;/em&gt; 가 할당되고, 각 &lt;em&gt;round&lt;/em&gt; 는 크게 3개의 비동기적인 &lt;em&gt;phase&lt;/em&gt; 로 분류할 수 있습니다.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;election:&lt;/strong&gt; a leader is elected&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;bill:&lt;/strong&gt; leader proposes a value, processes ack&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;law:&lt;/strong&gt; leader multicasts final value&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/election.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;먼저 &lt;em&gt;potential leader&lt;/em&gt; 가 &lt;em&gt;unique ballot id&lt;/em&gt; 를 고르고, 다른 프로세서들에게 보냅니다. 다른 프로세스들의 반응에 의해서 선출될 수도 있고, 선출되지 않으면 새로운 라운드를 시작합니다.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Because becoming a leader requires a majority of votes, and any two majorities intersect in at least one process, and each process can only vote once.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/bill.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;리더가 다른 프로세스들에게 &lt;code&gt;v&lt;/code&gt; 를 제안하고, 프로세스들은 지난 라운드에 &lt;code&gt;v&#39;&lt;/code&gt; 를 결정했었으면 &lt;code&gt;v=v&#39;&lt;/code&gt; 를 이용해 값을 결정합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/decision.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;만약 리더가 &lt;em&gt;majority&lt;/em&gt; 의 긍정적인 반응을 얻으면 모두에게 그 결정을 알리고 각 프로세서는 합의된 내용을 전달받고, 로그에 기록하게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/paxos_no_return.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;사실 이 과정은 응답을 리더가 받는 단계에서 결정되는 것이 아니라, 프로세서들이 &lt;em&gt;proposed value&lt;/em&gt; 를 듣는순간 결정됩니다. 따라서 리더에서 &lt;em&gt;failure&lt;/em&gt; 가 일어나도, 이전에 결정되었던 &lt;code&gt;v&#39;&lt;/code&gt; 을 이용할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/paxos_safety.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;이전에도 언급했듯이 &lt;em&gt;safety&lt;/em&gt; 는 두개의 서로 다른 프로세서의 의해서 다른 값이 선택되지 않음을 보장합니다. 이는 잠재적 리더가 있다 하더라도 현재 리더와, 잠재적 리더에게 응답하는 &lt;em&gt;majority&lt;/em&gt; (반수 이상) 을 교차하면 적어도 하나는 &lt;code&gt;v&#39;&lt;/code&gt; 를 응답하기 때문에 &lt;em&gt;bill phase&lt;/em&gt; 에서 정의한대로 이전 결과인 &lt;code&gt;v&#39;&lt;/code&gt; 가 사용됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/1ambda/1ambda.github.io/master/assets/images/cloud-computing-concept-1/week5/paxsos_liveness.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;그림에서 볼 수 있듯이 영원히 끝나지 않을수도 있지만, 실제로는 꽤 빠른시간 내에 합의에 도달합니다. (eventualy-live in async systems)&lt;/p&gt;

&lt;p&gt;&lt;br/&gt;&lt;/p&gt;

&lt;h3 id=&#34;refs&#34;&gt;Refs&lt;/h3&gt;

&lt;p&gt;(1) &lt;a href=&#34;http://ook.co/solutions/cloud-computing/&#34;&gt;Title Image&lt;/a&gt;&lt;br /&gt;
(2) &lt;strong&gt;Cloud Computing Concept 1&lt;/strong&gt; by &lt;em&gt;Indranil Gupta&lt;/em&gt;, Coursera&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>