<!DOCTYPE html>
<html itemscope itemtype="http://schema.org/Blog">
<head>
  <meta http-equiv="Content-Type" content="text/html" charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />

  <title>Machine Learning, Week 2</title>
  <meta name="description" content="" />

  <meta name="HandheldFriendly" content="True" />
  <meta name="MobileOptimized" content="320" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />

  

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Machine Learning, Week 2">
  <meta name="twitter:description" content="Machine Learning by Andrew Ng, Coursera Linear Regression with Multiple Variables Mutiple Features 변수가 적을때는 Hypothesis 가 간단하다. 많으면 어떻게 될까? Feature 가 N+1 개라면,    http://bt22dr.wordpress.com 편의상 x_0 = 1 이라 두면, Hypothesis 는 Zero-based index 인 n+1 벡터 h 와 x 의 곱이다. 따라서">
  <meta name="twitter:creator" content="@yourTwitterUsername">
  <meta name="twitter:image" content="">
  <meta name="twitter:url" content="http://1ambda.github.io/machine-learning-week-2/">
  <meta name="twitter:domain" content="http://1ambda.github.io">

  

  <link rel="author" href="https://plus.google.com/101105410053351451441?rel=author" />

  <link rel="shortcut icon" href="../favicon.ico">

  <link rel="stylesheet" type="text/css" href="http://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" />
  <link rel="stylesheet" type="text/css" href="http://fonts.googleapis.com/css?family=Droid+Serif" />
  <link rel='stylesheet' type='text/css' href="http://fonts.googleapis.com/css?family=Open+Sans:600,300" />
  <link rel="stylesheet" type="text/css" href="../assets/stylesheets/xpressio.css" />
  <link rel="stylesheet" type="text/css" href="../assets/1ambda/1ambda.css" />
  <script type="text/javascript" src="../assets/1ambda/modernizr.js">
  </script>
  <script type="text/javascript" src="../assets/1ambda/detectizr.min.js">
  </script>

  <!--load css if windows -->
  <script type="text/javascript">
    if (Modernizr.windows) {
      file = location.pathname.split( "/" ).pop();
      link = document.createElement( "link" );
      link.href = "/assets/1ambda/1ambda_windows.css";
      link.type = "text/css";
      link.rel = "stylesheet";
      link.media = "screen,print";
      document.getElementsByTagName("head")[0].appendChild( link );
    }
  </script>


  
  <link rel="stylesheet" href="../assets/highlight/styles/github.css">
<script src="../assets/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>


  <meta name="generator" content="Ghost 0.5" />
<link rel="alternate" type="application/rss+xml" title="Old Lisper" href="../rss">
<link rel="canonical" href="http://1ambda.github.io/machine-learning-week-2/" />

  
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-52181619-1', '1ambda.github.io');
  ga('send', 'pageview');
</script>

  
</head>
<body>

  
  <script src="../public/jquery.js?v=d88f7d3655"></script>

  

<header class="site_width text center padding_top_big margin_bottom_big">
  
  <h1 class="blog_title margin_bottom_small"><a href="http://1ambda.github.io">Old Lisper</a></h1>
  <h4 class="text book">Lisp, Emacs, Scala</h4>
  
  <div class="social border solid top_small bottom_small padding_medium">
  <h6 class="text book color c_black_medium without_margin padding_right_big"><a href="/about-me"><i class="fa fa-user"></i> <span class="margin_left_small desktop">About me</span></a></h6>
  <h6 class="text book color c_black_medium without_margin padding_right_big"><a href="http://kr.linkedin.com/in/1ambda" target="_blank"><i class="fa fa-linkedin-square"></i> <span class="margin_left_small desktop">Linkedin</span></a></h6>
  <h6 class="text book color c_black_medium without_margin"><a href="http://github.com/1ambda" target="_blank"><i class="fa fa-github"></i> <span class="margin_left_small desktop">GitHub</span></a></h6>
</div>

</header>

<main class="site_width" role="main">
  <article class="post tag-coursera tag-machine-lerning tag-gradient-descent tag-normal-equation">

    

    <header class="text center margin_bottom_medium">
      <h5 class="text book small uppercase color c_black_light margin_bottom_small">Posted in <a href="../tag/coursera">coursera</a>, <a href="../tag/machine-lerning">machine lerning</a>, <a href="../tag/gradient-descent">gradient descent</a>, <a href="../tag/normal-equation">normal equation</a></h5>
      <h1 class="margin_bottom_medium">Machine Learning, Week 2</h1>
      <h5 class="text book small uppercase color c_black_light margin_bottom_small"><time datetime="2014-10-08">Wednesday, October 08, 2014</time>
      <br/><br/>
       <a href="http://1ambda.github.io/machine-learning-week-2/#disqus_thread">Comment</a>
      </h5>
    </header>

    <section>
      <p><strong>Machine Learning</strong> by Andrew Ng, <em>Coursera</em></p>

<h2 id="linearregressionwithmultiplevariables">Linear Regression with Multiple Variables</h2>

<h3 id="mutiplefeatures">Mutiple Features</h3>

<p>변수가 적을때는 <em>Hypothesis</em> 가 간단하다. 많으면 어떻게 될까? <em>Feature</em> 가 <code>N+1</code> 개라면,</p>

<p><img src="http://bt22dr.files.wordpress.com/2013/05/04_2.png?w=300&amp;h=19" align="center" />  </p>

<p align="center"><a href='http://bt22dr.wordpress.com'>http://bt22dr.wordpress.com</a></p>

<p>편의상 <code>x_0 = 1</code> 이라 두면, <em>Hypothesis</em> 는 <em>Zero-based index</em> 인 <code>n+1</code> 벡터 <code>h</code> 와 <code>x</code> 의 곱이다. 따라서 <code>h(x) = h_t * x</code> 로 표기할 수 있다. 이걸 <strong>Mutivariate linear regression</strong> 이라 부른다.</p>

<h3 id="gradientdescentformultiplevariables">Gradient Descent for Multiple Variables</h3>

<p><em>Cost function</em> 은 다음과 같다. 변수의 subscript 는 <code>j</code> 번째 <em>Feature</em> 를, superscript 는 <code>i</code> 번째 데이터임을 말한다.</p>

<p><img src="http://bt22dr.files.wordpress.com/2013/05/04_6.png" align="center" />  </p>

<p align="center">(<a href='http://bt22dr.wordpress.com/'>http://bt22dr.wordpress.com/</a>)</p>  

<p><br/></p>

<p>다음은 <em>Gradient Descent</em> 알고리즘을 구하는 정의다.</p>

<p><img src="http://bt22dr.files.wordpress.com/2013/05/04_7.png?w=300&amp;h=104" align="center" />  </p>

<p align="center">(<a href='http://bt22dr.wordpress.com/'>http://bt22dr.wordpress.com/</a>)</p>  

<p><br/></p>

<p>따라서</p>

<p><img src="http://bt22dr.files.wordpress.com/2013/05/04_8.png?w=630&amp;h=354" align="center" />  </p>

<p align="center">(<a href='http://bt22dr.wordpress.com/'>http://bt22dr.wordpress.com/</a>)</p>

<h3 id="featurescaling">Feature Scaling</h3>

<p><em>Feature</em> 간 데이터 크기가 많이 차이가 나면, <em>Gradient Descent</em> 에서 등고선 간 간격이 좁으므로, <em>Global optima</em> 를 찾는데 오래걸릴 수 있다. 따라서 <em>Feature</em> 값을 <code>m</code> 으로 나누거나  -1 과 1 사이로 <em>scaliing</em> 할 수 있다. 거꾸로 말하면, <em>Feature scaling</em> 을 이용하면 <em>Gradient descent</em> 가 결과값을 더 빠르게 찾는다.</p>

<p><img src="http://i.stack.imgur.com/4RBjR.png" align="center" /></p>

<p>또한 <strong>Mean normalization</strong> 을 이용할 수 있는데, 모든 <em>feature</em> 에서 평균을 빼서, 평균을 0 으로 만드는 방법이다.</p>

<p>더 일반적인 방법은 <em>mean normalization</em> 을 하고, 거기에 <code>max-min</code> 또는 <em>standard deviation</em> 으로 나누는 방법이다.</p>

<h3 id="learningrate">Learning Rate</h3>

<p>디버깅 팁 중 하나는, 우리가 작성한 <em>Gradient descent</em> 알고리즘이 매 <em>interation</em> 마다 줄어들어야 한다는 것이다.</p>

<p><img src="http://d37rcl8t6g8sj5.cloudfront.net/wp-content/uploads/gradient_descent_error_by_iteration.png" align="center" />  </p>

<p align="center">(<a href='http://spin.atomicobject.com'>http://spin.atomicobject.com</a>)</p>

<p>그리고, 어느 지점에선가 <em>converged</em> 되는지 검사하기 위해 <em>automatic convergence test</em> 를 사용할 수 있다. 예를 들어 한 이터레이션에서, 10^-3 보다 적게 줄어드는지 검사한다거나.</p>

<p>만약에 <em>gradient descent</em> 값이 증가하면, 더 작은 <em>learning rate</em> 를 사용해라. 그렇다고 너무 작은 값을 사용하면 <em>gradient descent</em> 가 느리게 수렴할 수 있다. <em>learning rate</em> 가 너무 크면, 심지어 수렴하지 않을 수도 있다.</p>

<p>따라서 <em>learning rate</em> 를 <code>0.001</code>, <code>0.003</code>, <code>0.01</code>, <code>0.03</code>, <code>0.1</code>, <code>0.3</code>, <code>1</code> 처럼 작은 것부터 선택하되, 천천히 늘려가는 것이 좋다.</p>

<h3 id="polynomialregression">Polynomial Regression</h3>

<p>집값을 예측하기 위해 두개의 <em>feature</em>, <code>frontage</code> 와 <code>depth</code> 가 있다고 하자. 두 값을 곱해 <code>area</code> 라는 새로운 <em>feature</em> 를 만들면, <em>Hypothesis</em> 가 간단해진다. 따라서 기존의 <em>feature</em> 를 이용 할 수 있는지도 잘 알아보는 게 좋다.</p>

<p>자 이제, 집 값(Housing prices) 을 예측하기 위해 <em>Size(Area)</em> 라는 <em>feature</em> 를 이용한다 하자. <em>training set</em> 이 다음과 같을때, </p>

<p><img src="http://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr_files/Image.png" align="center" />  </p>

<p align="center"><a href='http://www.holehouse.org/mlclass'>http://www.holehouse.org/mlclass</a></p>  

<p><br/></p>

<p><em>hypothesis</em> 를 <em>quadratic</em> 로 세우면 어느 지점부터는 예측된 값이 감소하므로 <em>traning set</em> 과 일치하지 않는다. 따라서 <em>cubic</em> 다항식을 이용해 볼 수 있겠는데, <em>feature</em> 가 <code>size</code> 하나 뿐이므로, <em>hypothesis</em> 는 <code>size</code> 를 이용한 삼차식이 되겠다.</p>

<p><img src="http://www.holehouse.org/mlclass/04_Linear_Regression_with_multiple_variables_files/Image%20[10].png" align="center" />  </p>

<p align="center"><a href='http://www.holehouse.org/mlclass'>http://www.holehouse.org/mlclass</a></p>

<p>이 경우 <code>size</code> 하나로 3개의 <em>feature</em> 를 만들었으니, <em>scaling</em> 이 문제가 될 수 있다.</p>

<p>이 전에 앞서서 <em>feature</em> 가 두개인 <em>hypothesis</em> (quadratic) 은 말이 안된다고 했는데, 두개지만 <em>square</em> 모델을 사용하면 우리가 가진 <em>training set</em> 과 얼추 맞아 떨어지는 모델을 찾을 수 있다. 그림이 없어서 대충 식을 첨부하면,</p>

<p><code>h(x) = y0 + y1(size) + y2 * square(size)</code></p>

<p>여기서 <code>y</code> 는 강의에서 말하는 <code>0(theta)</code> 라 보면 된다.</p>

<h3 id="nomalequation">Nomal Equation</h3>

<p><em>gradient descent</em> 는 반복하면서 특정 값에 수렴해 가는 알고리즘 이었지만 <strong>normal equation</strong> 은 그냥 <code>J(0)</code> 식을 풀어버려 값을 찾아낸다.</p>

<p>예를 들어서 <code>J(0)</code> 가 <code>0(theta)</code> 에 대해  <em>quadratic</em> 이면, <code>0</code> 에 대해 미분해서 최저점을 찾아내면 된다. 문제는, <code>0</code> 가 여러개 일때, 모든 <code>0_j</code> 에 대해 <em>cost function</em> 을 풀어야 한다는 것이다. <em>partial derivative</em> 를 이용해서 해를 찾으면 된다.</p>

<p><img src="http://www.longhaiqiang.com/wp-content/uploads/2013/08/Snip20130817_44.png" align="center" />  </p>

<p align="center">(<a href='http://www.longhaiqiang.com/'>http://www.longhaiqiang.com/</a>)</p>  

<p><br/></p>

<p>행렬을 이용할 수도 있다. 자세한 건 강의 내용을 보자, <em>design matrix</em> 라고 부르는 <code>X</code> 를 만들어서 아래의 식을 구하면 된다. 사실 <code>X</code> 는 그냥 <em>feature</em> 들을 있는 그대로 행렬로 만들면 된다. 맨 앞에 <code>x0</code> 만 추가해서. </p>

<p><img src="http://www.longhaiqiang.com/wp-content/uploads/2013/08/Snip20130817_41.png" align="center" />  </p>

<p align="center">(<a href='http://www.longhaiqiang.com/'>http://www.longhaiqiang.com/</a>)</p>

<p>참고로, 저 식을 <em>Octave</em> 에서는 다음과 같이 계산한다.</p>

<pre><code class="octave">pinv(X`*X)*X`*y  
</code></pre>

<p><br/></p>

<p><em>normal equation</em> 을 이용할때는 <em>feature scaling</em> 을 하지 않아도 괜찮다. <em>gradient descent</em> 와 비교해 보자면,</p>

<blockquote>
  <p><strong>Gradient Descent:</strong> <br/>
  (1) <em>learning rate</em> 를 골라야 한다. 
  (2) <em>feature scaling</em> 을 해야할 필요가 있다. <br />
  (3) <em>interation</em> 을 해야하므로 알고리즘이 제대로 돌아가는지 체크해야할 필요가 있다. <br />
  (4) 대신 <code>n</code> 이 커도 잘 돌아간다.</p>
  
  <p><strong>Normal Equation:</strong> <br/>
  (1) <em>learning rate</em> 를 고를 필요가 없다. <br />
  (2) <em>feature scaling</em> 을 해야할 필요가 없다. <br />
  (3) <em>interation</em> 을 하지 않는다. <br />
  (4) <code>n</code> 이 커질경우 굉장히 느려지고 <code>(X^TX)^-1)</code> 을 계산해야 한다.</p>
</blockquote>

<p>따라서 <code>n</code> 이 너무 크지 않으면, 100~1000 정도까지는, <em>normal equation</em> 을 쓰는편이 낫다.</p>

<h3 id="nomalequationnoninvertibility">Nomal Equation Noninvertibility</h3>

<p>만약에, 우리가 가진 <code>X</code> 가 <em>non-invertible</em> 하다면 어떻게 될까? <em>invertible matrix</em> 란, 아래를 만족시키는 <code>B</code> 가 존재하는 행렬이다. <code>I</code> 는 <em>identity matrix</em> 다.</p>

<p><img src="http://upload.wikimedia.org/math/7/3/3/7334597613ae1773c19e1ed1289349db.png" align="center" />  </p>

<p align="center">(<a href='http://en.wikipedia.org/wiki/Invertible_matrix'>http://en.wikipedia.org/wiki/Invertible_matrix</a>)</p>

<p>만약 저런 <code>B</code> 가 존재하지 않아 <em>non-invertible</em> 한 행렬을 <strong>sigular matrix</strong>, <strong>degenerate matrix</strong> 라 부른다.</p>

<p>우리가 계산해야 할 행렬이 <em>non-invertible</em> 이라면, 두 가지 경우가 있을 수 있는데, </p>

<p>(1) Redundant features(linearly dependent) e.g <code>x1 = (3.28) * x2</code> <br />
(2) too many features e.g <code>m &lt;= n</code></p>

<p>이럴 때는 몇몇 <em>feature</em> 를 삭제하고, <em>regulaization</em> 을 하면 된다. </p>

<h3 id="costfunctionoctave">Cost Function: Octave</h3>

<p><em>cost function</em> 을 구현 해 보면</p>

<pre><code class="matlab">function J = costFunctionJ(X, y, theta)

m = size(X, 1) % number of training examples  
predictions= X * theta; % predictions of hypothesis on all m examples  
sqrErros = (predictions-y).^2;

J = 1 / (2*m) * sum(sqrErros);  
</code></pre>

<p><em>R</em> 이나 이런것들은 행렬연산이 참 쉬운것 같다.</p>

<h3 id="vectorization">Vectorization</h3>

<p><em>Vectorization</em> 을 이용하면, <code>for loop</code> 을 제거할 수 있는데, 예를 들어</p>

<p><img src="http://i.ytimg.com/vi/jRr2XuZOWB8/hqdefault.jpg" align="center" /></p>

<p>이건 행렬 곱셈이 한번에 이루어진다는 것을 이용한 방법이다. 따라서 <em>gradient descent</em> 알고리즘에서 <code>theta</code> 를 <code>for-loop</code> 으로 구하는 것이 아니라, <em>vectorization</em> 을 이용하면 한번에 계산할 수 있다.</p>

<p>이게 그림을 구하기가 어려운데, 아래첨자(sub-script) 를 이렇게 기술한다고 하자. <code>x_0</code> 그럼, <em>grandient descent</em> 알고리즘 식에서 <em>learning rate</em> 뒷부분이 <em>vector</em> 가 되는데 그 이유는 <code>theta</code> 와 마찬가지로 <code>j</code> 에 대한 나열이기 때문이다.</p>

<p><img src="http://2.bp.blogspot.com/-ZxJ87cWjPJ8/TtLtwqv0hCI/AAAAAAAAAV0/9FYqcxJ6dNY/s1600/gradient+descent+algorithm+OLS.png" align="center" /></p>

<p>구글에 검색하니까 1번으로 뜨는게 <em>vectorization(parallel computing)</em> 이더라. 병렬 연산에 많이 사용되나보다.</p>

<h3 id="refenrences">Refenrences</h3>

<p>(1) <a href="http://stats.stackexchange.com/questions/111467/is-it-necessary-to-scale-the-target-value-in-addition-to-scaling-features-for-re">StackExchange</a> <br />
(2) <a href="http://bt22dr.wordpress.com/">http://bt22dr.wordpress.com/</a> <br />
(3) <a href="http://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/">http://spin.atomicobject.com</a> <br />
(4) <a href="http://www.holehouse.org/mlclass/01_02_Introduction_regression_analysis_and_gr.html">http://www.holehouse.org/mlclass/</a> <br />
(5) <a href="http://www.longhaiqiang.com/">http://www.longhaiqiang.com/</a>  </p>
    </section>

    <footer>
      
      <section class="author_info margin_top_big">
        <div class="alignleft border rad_circle" style="height: 87px; width: 87px; background-image: url(http://www.gravatar.com/avatar/aa2032ba2302419e3c2ede54f1fbf687?d=404&amp;s=250); background-size: cover;"></div>
        <p class="margin_left_medium text small">Author</p>
        <p class="margin_left_medium text bold"><a href="http://language.is">1ambda</a></p>
        <p class="margin_left_medium text small">Lisp, Emacs, FP</p>
      </section>
      
    </footer>

    

    
    <div id="disqus_thread" class="margin_top_big"></div>
<script type="text/javascript">
  /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
  var disqus_shortname = '1ambda'; // required: replace example with your forum shortname
  var disqus_identifier = '40';
  var disqus_url = 'http://1ambda.github.io/machine-learning-week-2/';

  /* * * DON'T EDIT BELOW THIS LINE * * */
  (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>

    </article>
</main>


  
  <script src="../assets/fitvids/jquery.fitvids.js"></script>
<script>
$(document).ready(function(){
  // Target your .container, .wrapper, .post, etc.
  $("section").fitVids();
});
</script>


  <footer class="blog_info margin_top_big padding_medium text center">
    <h5 class="text book small">&copy; 2014 <a href="..">Old Lisper</a>. All rights reserved.</h5>
    <h5 class="text book small"><a href="https://github.com/dreyacosta/velox" target="_blank" class="text bold">Velox theme</a> by <a href="http://dreyacosta.com/">David Rey</a></h5>
    <h5 class="text book small">Proudly published with <a href="http://ghost.org"><span>Ghost</span></a></h5>

  </footer>

  
  <script type="text/javascript">
/* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
var disqus_shortname = '1ambda'; // required: replace example with your forum shortname

/* * * DON'T EDIT BELOW THIS LINE * * */
(function () {
 var s = document.createElement('script'); s.async = true;
 s.type = 'text/javascript';
 s.src = '//' + disqus_shortname + '.disqus.com/count.js';
 (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
 }());
</script>



  </body>
  </html>
